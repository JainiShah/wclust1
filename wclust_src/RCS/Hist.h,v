head	1.25;
access;
symbols;
locks; strict;
comment	@ * @;


1.25
date	2009.06.25.04.04.14;	author samn;	state Exp;
branches;
next	1.24;

1.24
date	2008.11.27.20.19.03;	author samn;	state Exp;
branches;
next	1.23;

1.23
date	2008.09.16.01.46.57;	author samn;	state Exp;
branches;
next	1.22;

1.22
date	2008.08.11.20.46.34;	author samn;	state Exp;
branches;
next	1.21;

1.21
date	2008.07.24.18.47.32;	author samn;	state Exp;
branches;
next	1.20;

1.20
date	2008.07.15.03.48.38;	author samn;	state Exp;
branches;
next	1.19;

1.19
date	2008.06.03.16.12.01;	author samn;	state Exp;
branches;
next	1.18;

1.18
date	2008.04.22.21.33.19;	author samn;	state Exp;
branches;
next	1.17;

1.17
date	2008.04.14.21.46.47;	author samn;	state Exp;
branches;
next	1.16;

1.16
date	2008.04.11.22.28.34;	author samn;	state Exp;
branches;
next	1.15;

1.15
date	2008.04.09.04.58.50;	author samn;	state Exp;
branches;
next	1.14;

1.14
date	2008.04.07.03.42.58;	author samn;	state Exp;
branches;
next	1.13;

1.13
date	2008.03.17.22.15.28;	author samn;	state Exp;
branches;
next	1.12;

1.12
date	2008.03.16.21.33.09;	author samn;	state Exp;
branches;
next	1.11;

1.11
date	2008.03.05.04.46.49;	author samn;	state Exp;
branches;
next	1.10;

1.10
date	2008.03.05.01.25.32;	author samn;	state Exp;
branches;
next	1.9;

1.9
date	2008.03.03.20.10.00;	author samn;	state Exp;
branches;
next	1.8;

1.8
date	2008.02.02.21.22.52;	author samn;	state Exp;
branches;
next	1.7;

1.7
date	2008.01.13.02.45.39;	author samn;	state Exp;
branches;
next	1.6;

1.6
date	2008.01.12.09.36.41;	author samn;	state Exp;
branches;
next	1.5;

1.5
date	2008.01.10.18.44.51;	author samn;	state Exp;
branches;
next	1.4;

1.4
date	2008.01.10.05.36.41;	author samn;	state Exp;
branches;
next	1.3;

1.3
date	2008.01.10.00.46.14;	author samn;	state Exp;
branches;
next	1.2;

1.2
date	2008.01.09.23.34.18;	author samn;	state Exp;
branches;
next	1.1;

1.1
date	2008.01.05.15.00.52;	author samn;	state Exp;
branches;
next	;


desc
@""
@


1.25
log
@allow saving cluster quality to .cl files even if not in user mode
@
text
@// $Id: Hist.h,v 1.24 2008/11/27 20:19:03 samn Exp $ 
#pragma once
#include "Cluster.h"
#include "WCMath.h"
#include "KDTree.h"
#include "ScopedTimer.h"
#include "Log.h"
#include <map>
#include <set>

//#define DO_TIMING

//binary logarithm
inline prob_t log2(prob_t d)
{
	static prob_t dl2 = log(2.0);
	return log(d) / dl2;
}

inline double log2(double d)
{
	static double dl2 = log(2.0);
	return log(d) / dl2;
}

prob_t Prob(int,int);

void InitProbs(int iMaxNumElems);

struct ProbInitFree
{
	ProbInitFree(int i);
	~ProbInitFree();
};

inline bool GZeroMinElem(float f1,float f2)
{
	return f1 > 0.0 && (f1 < f2 || f2 <= 0.0);
}

inline float GZeroMinElem(vector<float>& v)
{
	float fm = *std::max_element(v.begin(),v.end());
	int i=0,sz=v.size();
	for(;i<sz;i++)
		if(v[i]<fm && v[i]>0.0)
			fm=v[i];
	return fm;
}

inline float MinElem(vector<float>& v,bool bAllowZero)
{
	float fm = *std::max_element(v.begin(),v.end());
	int i=0,sz=v.size();
	for(;i<sz;i++)
		if(v[i]<fm && (bAllowZero || v[i]>0.0))
			fm=v[i];
	return fm;
}

inline int MinIdx(vector<float>& v,bool bAllowZero)
{
	float fm = *std::max_element(v.begin(),v.end());
	int i=0,sz=v.size(), minID = 0;
	bool bFound = false;
	for(;i<sz;i++)
	{
		if(v[i]<fm && (bAllowZero || v[i]>0.0))
		{
			bFound = true;
			fm=v[i];
			minID=i;
		}
	}
	if(!bFound) return -1;
	return minID;
}

struct Neighbor
{
	prob_t m_dist;
	int m_id;
	Neighbor(int id,prob_t dist)
		:m_id(id),m_dist(dist){}
	Neighbor()
		:m_id(0),m_dist(0){}
};

inline bool operator<(Neighbor& n1,Neighbor& n2)
{
	return n1.m_dist<n2.m_dist;
}

//kd tree from biopython used for continuous probability distribution estimates 
class KDTreeHist
{
	NSKDTree::KDTree* m_pTree;

	int m_iDims;
	int m_iNumElems;	
	prob_t m_dPiPow;
	prob_t m_dTop;
	prob_t m_dGamma;

	vector<float> m_vData;
	
	vector<float> m_vProbs;

public:
	
	KDTreeHist()
		:m_iDims(0),
		 m_iNumElems(0),
		 m_pTree(0)
	{
	}

	virtual ~KDTreeHist()
	{
		if(m_pTree) delete m_pTree;
	}

	prob_t Top(){ return m_dTop; }
	prob_t PiPow(){ return m_dPiPow; }
	prob_t Gam(){ return m_dGamma; }

	bool SetData(vector<float>& vFloat,vector<int>& vIDs,int iNumPoints,int iDims,int iCID,bool bNot=false)
	{
		if(!vFloat.size() || iNumPoints<1) return false;

		int iV = 0 , iC = 0;

		int iTotalVs = vFloat.size() / iDims;

		m_vData = vector<float>(iNumPoints*iDims);
		int j = 0 , k = 0;
		if(bNot)
		{
			for(iV=0;iV<vIDs.size();iV++)
			{
				if(vIDs[iV] != iCID && vIDs[iV]<1000)
				{
					int iD = 0;
					for(iD=0;iD<iDims;iD++)
					{
						m_vData[j++]=vFloat[iV*iDims+iD];
					}
				}
			}
		}
		else
		{
			for(iV=0;iV<vIDs.size();iV++)
			{
				if(vIDs[iV] == iCID)
				{
					int iD = 0;
					for(iD=0;iD<iDims;iD++)
					{
						m_vData[j++]=vFloat[iV*iDims+iD];
					}
				}
			}
		}

		m_iDims = iDims;
		
		if(m_pTree) delete m_pTree;
		
		m_pTree = new NSKDTree::KDTree(m_iDims,8,false);
		m_iNumElems = iNumPoints;

		const prob_t PI=3.14159265358979323846;

		m_dPiPow = pow((prob_t)PI,(prob_t)m_iDims/two);
		m_dTop = (one/(m_iNumElems-one));
		m_dGamma = Gamma(m_iDims/two+one);
		
		m_pTree->set_data(&m_vData[0],iNumPoints);

		return true;
	}

	bool SetData(int iDims,float* pData,int iNumPoints)
	{
		if(!pData || iNumPoints<1) return false;

		m_iDims = iDims;
		if(m_pTree) delete m_pTree;
		//m_pTree = new NSKDTree::KDTree(m_iDims,16,false);
		m_pTree = new NSKDTree::KDTree(m_iDims,8,false);
		m_iNumElems = iNumPoints;

		m_vData = vector<float>(iNumPoints*m_iDims);
		memcpy(&m_vData[0],pData,iNumPoints*m_iDims*sizeof(float));

		const prob_t PI=3.14159265358979323846;

		m_dPiPow = pow((prob_t)PI,(prob_t)m_iDims/two);
		m_dTop = (one/(m_iNumElems-one));
		m_dGamma = Gamma(m_iDims/two+one);
		
		m_pTree->set_data(&m_vData[0],iNumPoints);

	//	m_vProbs = vector<float>(iNumPoints);
		
		return true;
	}

	int NumElems()
	{
		return m_iNumElems;
	}

	int NumDims()
	{
		return m_iDims;
	}

	float* operator[](int i)
	{
		if(i >= m_iNumElems) return 0;
		
		return &m_vData[i*m_iDims];
	}

	float GetKNN(float* p,vector<Neighbor>& vnn,int iNNToFind,double fRadStart=8.0,double fRadFctr=2.0)
	{
		if(m_iNumElems == 1) return 0.0;
		
		int iIter = 0;
		float fRad = fRadStart;

		while(true)
		{
			m_pTree->search_center_radius_sq(p,fRad,iNNToFind);
			int iCount = m_pTree->get_count();
			if(iCount>iNNToFind)// || (iCount&&iNNToFind==1))
			{
				vector<float> vRadii(iCount);
				m_pTree->copy_radii_sq(&vRadii[0]);

				vector<long> vID(iCount);
				m_pTree->copy_indices(&vID[0]);

				vnn=vector<Neighbor>(iCount);

#ifdef _DEBUG
				Write2Log("found %d neighbors want %d",iCount,iNNToFind);
#endif

				int i = 0, j = 0;
				for(i=0;i<iCount;i++)
					if(vRadii[i]>0.0)
						vnn[j++] = Neighbor(vID[i],vRadii[i]);
				
				vnn.resize(j);
				std::sort(vnn.begin(),vnn.end());

				if(vnn.size()>iNNToFind)
				{
					vector<Neighbor> vNNTmp(iNNToFind);
					std::copy(vnn.begin(),vnn.begin()+iNNToFind,vNNTmp.begin());
					vnn=vNNTmp;
				}

#ifdef _DEBUG
				prob_t ttt = vnn[0].m_dist;
#endif
				
				return fRad;
			}
			//increase search radius
			fRad *= fRadFctr;
		}		
	}

	float GetKNN(float* p,Neighbor* vnn,int iNNToFind,double fRadStart,double fRadFctr)
	{
		if(m_iNumElems == 1) return 0.0;
		
		int iIter = 0;
		float fRad = fRadStart;

		while(true)
		{
			m_pTree->search_center_radius_sq(p,fRad,iNNToFind);
			int iCount = m_pTree->get_count();
			if(iCount>iNNToFind)// || (iCount&&iNNToFind==1))
			{
				vector<float> vRadii(iCount);
				m_pTree->copy_radii_sq(&vRadii[0]);

				vector<long> vID(iCount);
				m_pTree->copy_indices(&vID[0]);

				vector<Neighbor> vnnTMP(iCount);

				int i = 0, j = 0;
				for(i=0;i<iCount;i++)
					if(vRadii[i]>0.0)
						vnnTMP[j++] = Neighbor(vID[i],vRadii[i]);
				
				vnnTMP.resize(j);
				std::sort(vnnTMP.begin(),vnnTMP.end());

				for(i=0;i<iNNToFind;i++) vnn[i]=vnnTMP[i];

				return fRad;
			}
			//increase search radius
			fRad *= fRadFctr;
		}		
	}

	float GetAllKNN(A2D<Neighbor>& vnn,int iNNToFind,double fRadStart,double fRadFctr,vector<int>& vFound)
	{
		if(m_iNumElems == 1) return 0.0;
		
		int iIter = 0;
		float fRad = fRadStart;

		vnn.Fill(Neighbor(0,INF));
		vFound.resize(m_iNumElems);

		while(true)
		{
			m_pTree->neighbor_search_sq(fRadStart);
			int iCount = m_pTree->neighbor_get_count();
			if(iCount/2>m_iNumElems*iNNToFind)
			{
				Neighbor** pnn = vnn.GetP();

				vector<float> vRadii(iCount);
				m_pTree->neighbor_copy_radii_sq(&vRadii[0]);

				vector<long> vID(iCount*2);
				m_pTree->neighbor_copy_indices(&vID[0]);

				int i = 0 , j = 0, k = 0;
				for(i=0;i<iCount;i+=2)
				{
					int iN1 = vID[i], iN2 = vID[i+1], iPos = 0;
					
					prob_t distSQ = vRadii[i/2];
					
					if(distSQ <= 0.0f) continue;
					
					//find sorted position in iN1 neighbor array to place element in
					for(j=0;j<vFound[iN1];j++)
					{
						if(distSQ < pnn[iN1][j].m_dist)
						{
							iPos = j;
							break;
						}
					}
					//do we need to shift elements after this one?
					if(iPos < iNNToFind - 1)
					{
						for(j=vFound[iN1]-1;j>iPos;j--)
							pnn[iN1][j]=pnn[iN1][j-1];
						pnn[iN1][iPos]=Neighbor(iN2,distSQ);
						if(vFound[iN1]+1<iNNToFind)vFound[iN1]++;
					}
					else if(iPos<iNNToFind)//no shift needed, just store
					{	
						pnn[iN1][iPos]=Neighbor(iN2,distSQ);
						if(vFound[iN1]+1<iNNToFind)vFound[iN1]++;
					}

					iPos = 0;
					for(j=0;j<vFound[iN2];j++)
					{
						if(distSQ < pnn[iN2][j].m_dist)
						{
							iPos = j;
							break;
						}
					}
					if(iPos < iNNToFind - 1)
					{
						for(j=vFound[iN2]-1;j>iPos;j--)
							pnn[iN2][j]=pnn[iN2][j-1];
						pnn[iN2][iPos]=Neighbor(iN1,distSQ);
						if(vFound[iN2]+1<iNNToFind)vFound[iN2]++;
					}
					else if(iPos<iNNToFind)	
					{
						pnn[iN2][iPos]=Neighbor(iN1,distSQ);
						if(vFound[iN2]+1<iNNToFind)vFound[iN2]++;
					}
				}
				return fRad;
			}
			//increase search radius
			fRad *= fRadFctr;
		}		
	}

	//get nearest neighbor as float*
	float* GetNearestNeighbor(float* p,bool bAllowZeroDist)
	{
		if(m_iNumElems == 1) return 0;
		
		const int iNumRads = 7;
		float pRads[7] = {3.0f,30.0f,150.0f,300.0f,600.0f,900.0f,1000.0f};

		int iIter = 0;
		float fRad = pRads[0];

		while(true)
		{
			m_pTree->search_center_radius_sq(p,fRad,1);
			int iCount = m_pTree->get_count();
			if(iCount)
			{
				vector<float> vRadii(iCount);
				m_pTree->copy_radii_sq(&vRadii[0]);

				int id = MinIdx(vRadii,bAllowZeroDist);
				if(id != -1)
				{
					vector<long> vID(iCount);
					m_pTree->copy_indices(&vID[0]);
					return &m_vData[vID[id]*m_iDims];
				}
			}
			//increase search radius
			if(iIter+1 >= iNumRads)
				fRad *= two;
			else
				fRad = pRads[++iIter];
		}		
	}

	float GetNearestRadiusSQ(float* p,vector<int>& vMap,int iID)
	{
		if(m_iNumElems == 1) return 0.0;
		
		int iIter = 0;
		extern prob_t gstartrad;
		float fRad = gstartrad;

		while(true)
		{
			m_pTree->search_center_radius_sq(p,fRad,1);
			int iCount = m_pTree->get_count();
			if(iCount)
			{
				vector<float> vRadii(iCount);
				m_pTree->copy_radii_sq(&vRadii[0]);

				float fm = FLT_MAX; 

				vector<long> vIndices(iCount);
				m_pTree->copy_indices(&vIndices[0]);

				bool bFound = false;

				int i = 0;
				for(i=0;i<iCount;i++)
				{
					if(vMap[vIndices[i]]==iID && vRadii[i]<= fm && vRadii[i]>0.0)
					{	bFound = true;
						fm = vRadii[i];
					}
				}

				if(bFound)
					return fm;
			}
			//increase search radius
			fRad *= two;
		}		
	}

	bool GetNearestNeighbor(float* p,bool bAllowZeroDist,Neighbor& n)
	{
		if(m_iNumElems == 1) return false;
		m_pTree->search_nn(p,bAllowZeroDist);
		float fm = 0.0;
		m_pTree->copy_radii_sq(&fm);
		long id;
		m_pTree->copy_indices(&id);
		n.m_dist = fm;
		n.m_id = id;
		return true;
	}

	float GetNearestRadiusSQ(float* p,bool bAllowZeroDist,bool bTest)
	{
		if(m_iNumElems == 1) return 0.0;		

		int iIter = 0;
		extern prob_t gstartrad;
		float fRad = gstartrad;

		if(bTest)
		{
#ifdef DO_TIMING
			extern MTimer oMT;	TimerInc oT(oMT);
#endif
			m_pTree->search_nn(p,bAllowZeroDist);
			float fm = 0.0;
			m_pTree->copy_radii_sq(&fm);
			return fm;
		}
		else
		{
#ifdef DO_TIMING
			extern MTimer oMF; TimerInc oT(oMF);
#endif
			while(true)
			{
				m_pTree->search_center_radius_sq(p,fRad,1);
				int iCount = m_pTree->get_count();
				if(iCount)
				{
					vector<float> vRadii(iCount);
					m_pTree->copy_radii_sq(&vRadii[0]);

					float fm = 0.0;

					if(bAllowZeroDist)
						return *std::min_element(vRadii.begin(),vRadii.end());
					else 
						fm=GZeroMinElem(vRadii);

					if(fm>0.0)
						return fm;
				}
				//increase search radius
				fRad *= two;
			}
		}
	}

	float GetNearestRadiusSQ(int i,bool bTest)
	{
		return GetNearestRadiusSQ(&m_vData[i*m_iDims],false,bTest);
	}

	//returns probability based on distance
	//of an arbitrary element in THIS distribution
	//to it's nearest neighbor in THIS distribution
	prob_t RProb(prob_t dRad)
	{
		return m_dTop / (m_dPiPow*dRad*m_dGamma);
	}

	//returns probability based on distance
	//of an arbitrary element in a DIFFERENT distribution
	//to it's nearest neighbor in THIS distribution
	prob_t RProbOther(prob_t dRad)
	{
		return (one/m_iNumElems) / (m_dPiPow*dRad*m_dGamma);
	}

	//returns probability of element i
	prob_t IProb(int i)
	{
		if(!m_pTree || i<0 || i>=m_iNumElems) return 0.0;

		if(1==m_iNumElems)return 1.0;

		return VProb(&m_vData[i*m_iDims]);
	}

	//returns probability of vector p
	//vector p must be in THIS distribution
	prob_t VProb(float* p)
	{
		if(!p || !m_pTree) return 0.0;

		if(1==m_iNumElems)return 1.0;

		prob_t dRad = sqrt(GetNearestRadiusSQ(p,false,false));

		return RProb(dRad);
	}

	//returns probability of vector p
	//vector p must be in DIFFERENT distribution
	prob_t VProbOther(float* p)
	{
		if(!p || !m_pTree) return 0.0;

		if(1==m_iNumElems)return 1.0;

		prob_t dRad = sqrt(GetNearestRadiusSQ(p,true,false));

		if(dRad == 0.0) return 0.0;

		return RProbOther(dRad);
	}

	char m_strMsg[1024];

	//entropy of distribution
	prob_t Entropy()
	{	//sprintf(m_strMsg,"Entropy sz=%d",m_iNumElems);
		//ScopedTimer S(m_strMsg);
		if(m_iNumElems<2) return 0.0;
		
		prob_t dEntrop = 0.0;
		prob_t dPiPowGamma = m_dPiPow*m_dGamma;

		int isz = m_iNumElems , i=0, iOffset = 0;
		
		for(i=0;i<isz;i++)
		{
			prob_t dDist = GetNearestRadiusSQ(&m_vData[iOffset],false,false);
			if(dDist<=0.0)continue;
			dDist = sqrt(dDist); 
			prob_t dProb = m_dTop / (dDist*dPiPowGamma);
			if(dProb<=0.0)continue;
			dEntrop += dProb * log2(dProb);
			iOffset += m_iDims;
		}
		return -dEntrop;
	}

	//vIDs specifies which cluster each element
	//belongs to. iClust specifies which cluster
	//to get entropy for
	prob_t Entropy(vector<int>& vIDs,int iClust)
	{
		prob_t dEntrop = 0.0;
		int isz = m_iNumElems , i=0;
		for(i=0;i<isz;i++)
		{
			if(vIDs[i]==iClust)
			{
				prob_t dProb = IProb(i);
				if(dProb==0.0) continue;
				dEntrop += dProb * log2(dProb);
			}
		}
		return -dEntrop;
	}

	//vIDs specifies which cluster each element
	//belongs to. iClust specifies which cluster
	//to get entropy for
	prob_t Entropy(vector<int>& vIDs,int iClust,int iNumElems)
	{	sprintf(m_strMsg,"Entropy c%d sz=%d totsz=%d",iClust,iNumElems,m_iNumElems);
		ScopedTimer S(m_strMsg);
		if(iNumElems<2) return 0.0;

		const prob_t PI=3.14159265358979323846;
		prob_t dPiPow = pow((prob_t)PI,(prob_t)m_iDims/two);
		prob_t dTop = (one/(iNumElems-one));
		prob_t dGamma = Gamma(m_iDims/two+one);
		prob_t dPiPowGamma = dPiPow*dGamma;

		prob_t dEntrop = 0.0;
		int isz = m_iNumElems , i=0;
		for(i=0;i<isz;i++)
		{
			if(vIDs[i]==iClust)
			{
				prob_t dDist = GetNearestRadiusSQ(&m_vData[i*m_iDims],vIDs,iClust);
				if(dDist<=0.0)continue;
				dDist=sqrt(dDist);
				prob_t dProb = dTop / (dDist*dPiPowGamma);
				if(dProb<=0.0)continue;
				dEntrop += dProb * log2(dProb);
			}
		}
		return -dEntrop;
	}
};

template< class T >
struct vpwrap
{
	vector<T>* p_;
	int iMax_, iMin_;
	
	vpwrap(vector<T>* p,int iMin=-1,int iMax=-1)
		:p_(p),
	     iMax_(iMax),
		 iMin_(iMin){}

	//global operator== is used, not this one
    //this one doesn't even get compiled
	bool operator==(const vpwrap<T>& vr) const
	{
		if(iMax_ == -1 || iMin_ == -1) 
			return *p_ == *vr.p_;
		
		dont
		
		vector<T>& v1 = *p_;
		vector<T>& v2 = *vr.p_;
		
		int i = iMin_;
		for(;i<iMax_;i++)
			if(v1[i] != v2[i]) 
				return false;
		
		return true;
	}

	bool operator<(const vpwrap<T>& vr) const
	{
		if(iMax_ == -1 || iMin_ == -1) 
			return *p_ < *vr.p_;

		vector<T>& v1 = *p_;
		vector<T>& v2 = *vr.p_;

		return lexicographical_compare(&v1[iMin_],&v1[iMax_],&v2[iMin_],&v2[iMax_]);
	}

	T Dist(const vpwrap<T>& vr, T tMinSoFar) const
	{
		vector<T>& v1 = *p_;
		vector<T>& v2 = *vr.p_;

		int iMinT = iMin_ == -1 ? 0 : iMin_,
			iMaxT = iMax_ == -1 ? v1.size() : iMax_;

		T tDist(0);
		
		int i = iMinT;

		for(;i<iMaxT;i++)
		{
			T val = v1[i] - v2[i];
			val *= val;
			tDist += val;
			if(tDist > tMinSoFar) return tDist;
		}
		
		return tDist;
	}

	void Print(FILE* fp=stdout,bool bNewLine=false) const
	{
		vector<T>& v = *p_;		
		
		int iMinT = iMin_ == -1 ? 0 : iMin_;
		int iMaxT = iMax_ == -1 ? v.size() : iMax_;
		int i = iMinT;
		
		if(sizeof(T)==sizeof(int))
		{
			for(;i<iMaxT;i++)
			{
				fprintf(fp," %d ",v[i]);
			}
		}
		else// if(sizeof(T)==sizeof(float))
		{
			for(;i<iMaxT;i++)
			{
				fprintf(fp," %f ",v[i]);
			}
		}
		if(bNewLine) fprintf(fp,"\n");
	}
};

inline bool operator==(const vpwrap<float>& vl,const vpwrap<float>& vr)
{
	if(vl.iMax_ == -1 || vl.iMin_ == -1) 
		return *vl.p_ == *vr.p_;
	
	vector<float>& v1 = *vl.p_;
	vector<float>& v2 = *vr.p_;
	
	int i = vl.iMin_;
	for(;i<vl.iMax_;i++)
		if(v1[i] != v2[i]) 
			return false;
	
	return true;
}

inline bool operator==(const vpwrap<int>& vl,const vpwrap<int>& vr)
{
	if(vl.iMax_ == -1 || vl.iMin_ == -1) 
		return *vl.p_ == *vr.p_;
	
	vector<int>& v1 = *vl.p_;
	vector<int>& v2 = *vr.p_;
	
	int i = vl.iMin_;
	for(;i<vl.iMax_;i++)
		if(v1[i] != v2[i]) 
			return false;
	
	return true;
}

typedef std::map< vpwrap<int> , int> THMapI;
typedef THMapI::iterator THMapITI;

typedef std::map< vpwrap<float> , int> THMapF;
typedef THMapF::iterator THMapITF;

template< class T >
class TreeHist
{
	typename std::map< vpwrap<T> , int> m_hist;
	int m_iNumDims;
	int m_iNumElems;
	bool m_bBinless;
	int m_iMinD;
	int m_iMaxD;

public:

	void SetDRange(int iMinD,int iMaxD){ m_iMinD=iMinD; m_iMaxD=iMaxD; }

	bool GetBinless(){ return m_bBinless; }
	void SetBinless(bool b){ m_bBinless=b; }
	
	typename std::map< vpwrap<T> , int>::iterator Begin(){ return m_hist.begin(); }
	typename std::map< vpwrap<T> , int>::iterator End(){ return m_hist.end(); }
	int Size(){ return m_hist.size(); }

	void Print(FILE* fp=stdout)
	{
		int i = 0;
		std::map< vpwrap<T> , int>::iterator IT = m_hist.begin();
		for(;IT!=m_hist.end();IT++)
		{
			fprintf(fp,"vec%d  ",i++);
			IT->first.Print(fp,true);
			fprintf(fp,"count = %d, probability = %.4f\n\n",IT->second,GetITProb(IT));
		}
	}

	prob_t SumProb()
	{
		if(m_iNumElems == 0) return 0.0;

		prob_t dSum = 0.0;
		std::map< vpwrap<T> , int>::iterator IT = Begin();
		for(;IT!=End();IT++)
		{
			dSum += IT->second;
		}
		dSum /= (prob_t) m_iNumElems;
		return dSum;
	}

	bool UpdateProb( vector<T>* v,int iCount)
	{
		vpwrap<T> vr(v,m_iMinD,m_iMaxD);
		std::map< vpwrap<T> , int>::iterator IT = m_hist.find(vr);
		if(IT==m_hist.end())
		{
			m_hist.insert( make_pair(vr,iCount) );
			m_iNumElems += iCount;
		}
		else
		{
#ifdef _DEBUG
			vector<T>& v1 = *v;
			vector<T>& v2 = *IT->first.p_;
			int moo=0;
#endif
			IT->second+=iCount;
			m_iNumElems += iCount;

			if(IT->second == 0)
				m_hist.erase(IT);
		}
		return true;
	}

	//get count of bin that dVal is in 
	inline int GetVCount( vector<int>* v)  
	{ 
		typename std::map< vpwrap<T> , int>::iterator IT = m_hist.find(vpwrap<T>(v,m_iMinD,m_iMaxD));
		if(IT==m_hist.end())
			return 0;
		return IT->second;
	}

	//get probability of variable being bin i
	inline prob_t GetVProb(vector<int>* v) 
	{
		if(m_iNumElems == 0) return 0.0;
		return GetVCount(v) / (float) m_iNumElems; // Prob(m_iNumElems,GetVCount(v)); 
	}

	inline prob_t GetITProb(typename std::map< vpwrap<T> , int>::iterator& IT)
	{
		return IT->second / (float) m_iNumElems; // Prob(m_iNumElems,IT->second);
	}

	inline prob_t Entropy()
	{
		int i = 0;
		typename std::map< vpwrap<T> , int>::iterator IT = m_hist.begin();
		prob_t dEntropy = 0.0;

		if(m_bBinless)
		{
			if(1==m_hist.size())return 0.0;
			prob_t dSz = m_hist.size();
			prob_t dDim = Begin()->first.iMax_ - Begin()->first.iMin_ + 1.0;
			prob_t dPiPow = pow(PI,(prob_t)dDim/2.0);
			prob_t dTop = (1.0/(dSz-1.0));
			prob_t dGamma = Gamma(dDim/2.0+1.0);
			for(;IT!=m_hist.end();IT++)
			{
				T tDistC = sqrt( (prob_t) ClosestDist(IT));
				if(tDistC == 0.0) continue;
				prob_t dProb = (IT->second*dTop) / (dPiPow*tDistC*dGamma);
				dEntropy += dProb * log2(dProb);
			}
		}
		else
		{
			for(;IT!=m_hist.end();IT++)
			{
				prob_t dProb = IT->second / (prob_t) m_iNumElems; // Prob(m_iNumElems,IT->second); 
				if(dProb == 0.0) continue;
				dEntropy += dProb * log2(dProb);
			}
		}
		return -dEntropy;
	}
	
	TreeHist(){ m_iNumElems = 0; m_bBinless = false; m_iMinD=-1;m_iMaxD=-1;};
	~TreeHist(){};

	//add h to this
	void Add(TreeHist& h)
	{
		typename std::map< vpwrap<T> , int>::iterator IT = h.m_hist.begin();
		for(;IT!=h.m_hist.end();IT++)
		{
			UpdateProb(IT->first.p_,IT->second);
		}
	}

	T ClosestDist(typename const std::map< vpwrap<T>, int>::iterator ITOther)
	{
		T tMinSoFar = 9e10;
		
		typename std::map< vpwrap<T>, int>::iterator IT = Begin();
		
		for(;IT!=End();IT++)
		{
			if(IT == ITOther) continue;

			T tDistTmp = IT->first.Dist(ITOther->first,tMinSoFar);
			
			if(tDistTmp < tMinSoFar)
				tMinSoFar = tDistTmp;
		}
		return tMinSoFar;
	}
};

//histogram with variable bin width
class VHist
{
	float m_fMin;
	float m_fMax;
	float m_fBinWidth;
	int   m_iBins;
	float m_fNumElems;
	vector<int> m_counts;

	bool ChangeBin(float val,int iInc)
	{
		int ibin = Bin(val);
		if(ibin<0 || ibin>=m_iBins)
			return false;
		m_counts[ibin]+=iInc;
		m_fNumElems+=iInc;
		return true;
	}

public:

	float BinWidth()
	{
		return m_fBinWidth;
	}

	float Elems()
	{
		return m_fNumElems;
	}

	int Bins()
	{
		return m_iBins;
	}

	int operator[](int i)
	{
		return m_counts[i];
	}

	int Bin(float val)
	{	//(-1.1 - -1.2) / .5 = .1 / .5 = 1/5 = 0
	    //(-0.6 - -1.2) / .5 = .6 / .5 = 6/5 = 1
		return ( val - m_fMin) / m_fBinWidth;
	}

	float Prob(float val)
	{
		try
		{
			return m_counts[Bin(val)] / m_fNumElems;
		}
		catch(...)
		{	Write2Log("VHist::Prob exception!");
			return 0.0f;
		}
	}

	float ProbSum()
	{	int i = 0;
		float s = 0.0;
		for(;i<m_iBins;i++)
			s += m_counts[i] / m_fNumElems;
		return s;
	}

	float Entropy()
	{	int i = 0;
		float e = 0.0 , p = 0.0;
		for(;i<m_iBins;i++)
		{	p = m_counts[i] / m_fNumElems;
			if(p<=0.0) continue;
			e -= p * log2(p);
		}
		return e;
	}

	bool Inc(float val)
	{
		return ChangeBin(val,1);
	}

	bool Dec(float val)
	{
		return ChangeBin(val,-1);
	}
	
	bool Init(float fMin,float fMax,float BinWidth)
	{
		m_fMin=fMin; m_fMax=fMax; m_fBinWidth=BinWidth;
		m_fNumElems=0.0f;
		m_iBins = (m_fMax-m_fMin)/m_fBinWidth;
		if(!m_iBins) return false;
		m_counts=vector<int>( m_iBins , 0);
		return true;
	}

	VHist(float fMin,float fMax,float BinWidth)
	{
		Init(fMin,fMax,BinWidth);
	}

};
#include <hash_map>
struct eqp
{
  bool operator()(const int* s1, const int* s2) const
  {
    return memcmp(s1, s2, sizeof(int)*2 );
  }
};
//histogram class, with automatic counting/scaling
//of values to be between min & max
//uses fixed bin width, so bin 0 corresponds to min value
//and last bin corresponds to max value. should only use
//to compare distributions with same min and max, otherwise
//values will be squeezed together into bins differently
typedef double hprob;
class Hist
{
protected:
	
	//counts of values
	std::vector<int> m_counts;

	//vector of probability values, has probs for (0 - m_iNumElems) / m_iNumElems
	std::vector<hprob> m_probs;
	
	//min value in histogram
	hprob m_dMin;
	
	//max value in histogram
	hprob m_dMax;

	//m_dMax - m_dMin
	hprob m_dRange;
	
	//number of bins
	int m_iBins;

	//# of elements in distribution
	hprob m_dNumElems;

	//whether need to recalc probabilities
	//(uses lazy evaluation)
	bool m_bNeedReCalc;

	hash_map< int* ,hprob, hash_compare<const int*, eqp> > m_cache;
	hash_map< int* ,hprob, hash_compare<const int*, eqp> >::iterator m_IT;

public:

	//return bin # for value
	inline int ValIndex(hprob dVal) const
	{
		if(m_dRange==0.0) return 0;

		//dVal's location in range as # btwn 0 - 1
		hprob dFctr = (dVal - m_dMin) / m_dRange;
	
		//multiply by # of bins - 1 to have it 0-based index
		//doesn't use rounding to closest bin, but probably should
		return dFctr * (m_iBins - 1);
	}
	
	inline hprob Max() const { return m_dMax; }
	inline hprob Min() const { return m_dMin; }
	inline hprob Range() const { return m_dRange; }
	inline int NumBins() const { return m_iBins; }

	//number of elements in "distribution"
	inline int NumElems() const
	{
		return (int) m_dNumElems;
	}

	//increment bin by 1
	inline bool DecBin(int idx)
	{
#ifdef _DEBUG
		if(idx < 0 || idx >= m_counts.size()){ Write2Log("out of bounds %d %d!!!!\n",idx,m_counts.size());
			return false;}
#endif
		m_dNumElems -= 1;
		//m_bNeedReCalc = true;
		m_counts[idx] -= 1;	
		return true;
	}

	//increment bin by 1
	inline bool IncBin(int idx)
	{
#ifdef _DEBUG
		if(idx < 0 || idx >= m_counts.size()){ Write2Log("out of bounds %d %d!!!!\n",idx,m_counts.size());
			return false; }
#endif
		m_dNumElems += 1;
		//m_bNeedReCalc = true;
		m_counts[idx] += 1;	
		return true;
	}

	//increment bin that dVal is in by 1
	inline bool IncBinVal(hprob dVal)
	{
		if(dVal < m_dMin || dVal > m_dMax)
		{
			return false;
		}
		m_dNumElems += 1;
		m_bNeedReCalc = true;
		m_counts[ValIndex(dVal)] += 1;	
		return true;
	}

	//decrement bin that dVal is in by 1
	inline bool DecBinVal(hprob dVal)
	{
		if(dVal < m_dMin || dVal > m_dMax)
		{
			return false;
		}	
		m_dNumElems -= 1;
		m_bNeedReCalc=true;
		m_counts[ValIndex(dVal)] -= 1;	
		return true;
	}
	
	//set bin that dVal is in to iCount
	inline bool SetBinVal(hprob dVal,int iCount)
	{
		if(dVal < m_dMin || dVal > m_dMax)
		{
			return false;
		}
		int iDx = ValIndex(dVal);
		int iTmp = m_counts[iDx];
		m_dNumElems -= iTmp;
		m_dNumElems += iCount;
		m_counts[iDx] = iCount;
		m_bNeedReCalc = iCount != iTmp;
		return true;
	}
	
	//get count of bin that dVal is in 
	inline int GetBinVal(hprob dVal) const { return m_counts[ValIndex(dVal)]; }

	//get probability of variable being bin i
	inline hprob GetBinProb(hprob dVal) const{ return (hprob) GetBinVal(dVal) / m_dNumElems; }

	//calc probabilities of 0 - m_dNumElems to help save time
	//only recalculate when the # of elements changes
	//not used
	inline void CalcProbs()
	{
		if(m_bNeedReCalc && m_dNumElems>=1.0)
		{
			m_probs = vector<hprob>((int)m_dNumElems+1);
			hprob j;
			for(j=0.0;j<=m_dNumElems;j+=1.0) m_probs[j] = j / m_dNumElems;
			m_bNeedReCalc=false;
		}
	}

	//get probability of bin i
	inline hprob BinProb(int i)
	{ 
		return m_counts[i] / m_dNumElems; // Prob(m_dNumElems,m_counts[i]); 
	}

	inline hprob CachedEntropy()
	{
		int i = 0;
		hprob dEntropy = 0.0;
		hprob dC;
		int tmp[2] , iSZ = m_dNumElems;
		tmp[1] = iSZ;
		for(;i<m_counts.size();i++)
		{
			if(!m_counts[i]) continue;
			tmp[0] = m_counts[i];			
			if( (m_IT = m_cache.find(tmp)) == m_cache.end() )
			{	hprob dProb = m_counts[i] / m_dNumElems;
				dC = dProb * log2(dProb);								
				m_cache[tmp] = dC;
			}
			else dC = m_IT->second;
			dEntropy += dC;
		}
		return -dEntropy;
	}

	inline hprob Entropy()
	{
		int i = 0;
		hprob dEntropy = 0.0;
		for(;i<m_counts.size();i++)
		{
			if(!m_counts[i]) continue;
			hprob dProb = BinProb(i);
			dEntropy += dProb * log2(dProb);
		}
		return -dEntropy;
	}
	
	//direct access to bin
	const int& operator[] (int i) const { return m_counts[i]; }
	
	//initialize histogram
	bool Init(hprob dMin,hprob dMax,int iBins);

	Hist(void);
	Hist(hprob dMin,hprob dMax,int iBins);
	~Hist(void);

	//add h to this
	void Add(Hist& h)
	{
		int i;
		for(i=0;i<m_iBins;i++)
		{
			m_counts[i] += h.m_counts[i];
			m_dNumElems += h.m_counts[i];
		}
	}

	inline void Print(FILE* fp)
	{
		int i;
		for(i=0;i<m_iBins;i++)
		{
			fprintf(fp,"B%d : ",i);
			int j;
			fprintf(fp,"%d p=%0.2f ",m_counts[i],BinProb(i));
			for(j=0;j<m_counts[i];j++)
			{
				fprintf(fp,"*");
			}
			fprintf(fp,"\n");
		}
		fprintf(fp,"numelems=%d\n\n",NumElems());
	}
};

//max entropy with option of zeroing out certain dimensions
//if vZero[i] != 0, skip that dimension
inline prob_t MaxEntropy(vector< Hist >& vHist, vector<int>& vZero)
{
	int iSz = vHist.size();
	int i;
	prob_t dMax = 0.0 , dTmp = 0.0;
	for(i=0;i<iSz;i++)
	{
		if(vZero[i]) continue;
		if( (dTmp=vHist[i].Entropy()) > dMax )
			dMax = dTmp;
	}
	return dMax;
}

//sum entropy with option of zeroing out certain dimensions
//if vZero[i] != 0, skip that dimension
inline hprob SumEntropy(vector< Hist >& vHist, vector<int>& vZero)
{
	int iSz = vHist.size();
	int i;
	hprob dSum = 0.0;
	for(i=0;i<iSz;i++)
	{
		if(vZero[i]) continue;
		dSum += vHist[i].Entropy();
	}
	return dSum;
}

inline hprob SumEntropy(vector< Hist> & vHist)
{
	int iSz = vHist.size();
	int i;
	hprob dSum = 0.0;
	for(i=0;i<iSz;i++)
		dSum += vHist[i].Entropy();
	return dSum;
}

inline hprob SumEntropy(vector< vector<Hist> >& vDistribs,vector<prob_t>& vWeights)
{
	int iSz = vDistribs.size();
	int i;
	hprob dSum = 0.0;
	for(i=0;i<iSz;i++)
		dSum += SumEntropy(vDistribs[i]);
	return dSum;
}

inline void CalcEntropy(vector< vector< Hist> >& vDistribs,vector<hprob>& vcEntropy,int iClusts,int iDim,vector<int>& vCounts,int iNumElems,vector<int>& vZeroes)
{
	int iC;
	if(vcEntropy.size() < iClusts+1) vcEntropy = vector<hprob>(iClusts+1,0.0);
	for(iC=1;iC<=iClusts;iC++)
	{	
		vcEntropy[iC] = ( (hprob) vCounts[iC] / (hprob) iNumElems ) * SumEntropy(vDistribs[iC],vZeroes);
	}
	if(vCounts[0]) vcEntropy[0] = -1.0 * ( (hprob) vCounts[0] / (hprob) iNumElems ) * SumEntropy(vDistribs[0],vZeroes); // ************* //
}

template< class T >
inline void CalcEntropy(vector< TreeHist<T> >& vDistribs,vector<prob_t>& vcEntropy,int iClusts,vector<int>& vCounts,int iNumElems,bool bWeight)
{
	int iC;
	if(vcEntropy.size() < iClusts+1) vcEntropy = vector<prob_t>(iClusts+1);
	if(bWeight)
	{
		for(iC=1;iC<=iClusts;iC++)
		{
			vcEntropy[iC] = ( (prob_t) vCounts[iC] / (prob_t) iNumElems ) * vDistribs[iC].Entropy();
		}
	}
	else
	{
		for(iC=1;iC<=iClusts;iC++)
		{
			vcEntropy[iC] = vDistribs[iC].Entropy();
		}
	}
}


inline prob_t FullProb(Hist& h)
{
	prob_t p = 0.0;
	for(int i=0;i<h.NumBins();i++)
	{
		p += h.BinProb(i);
	}
	return p;
}

//get vector containing information gain for each cluster
//std::vector<prob_t> GetClusterInfo(CVerxStack& DataStack,CCluster& MainClusters,CPaletteStack* pMainPal,CFile* pFileBPF,int iBins,int iClusts,int which);
//get string rep of cluster info gain for writing to bpf,cl files
CString GetClusterInfoString(CVerxStack& MainDataStack,CCluster& MainClusters,HWND wnd=0,int DrawMode=CLUST_USER);

bool PrintDistribs(vector<vector< Hist> >& vDistribs,char* fname);

bool Distribs2Images(vector< vector< Hist > >& vDistribs, char* fname_base);

bool Distribs2Matlab(vector< vector< Hist > >& vDistribs, char* fname_base);

void FillDistribs(CVerxStack& DataStack,CCluster& MainClusters,int iBins,std::vector< std::vector<Hist> >& vDistribs,int iClusts,int which);

//this is the continuous multidimensional probability version
inline void FillDistribs(vector<float>& vFloat,vector<KDTreeHist>& vDistribs,vector<KDTreeHist>& vCompDistribs,int iDistribs,vector<int>& vClustIDs,vector<int>& vCounts,int iDims,bool bGetComplements)
{
	vDistribs = vector< KDTreeHist >(iDistribs+1);

	if(bGetComplements)
		vCompDistribs = vector< KDTreeHist >(iDistribs+1);

	int iV = 0 , iC = 0;

	//full distribution
	vDistribs[iDistribs].SetData(iDims,&vFloat[0],vFloat.size()/iDims);

	int iTotalVs = vFloat.size() / iDims;

	for(iC=1;iC<iDistribs;iC++)
	{
		vector<float> vClustData(vCounts[iC]*iDims), vCompData;
		int iCompSize = iTotalVs - vCounts[iC];
		if(bGetComplements) vCompData = vector<float>(iCompSize*iDims);
		int j = 0 , k = 0;
		for(iV=0;iV<vClustIDs.size();iV++)
		{
			if(vClustIDs[iV] == iC)
			{
				int iD = 0;
				for(iD=0;iD<iDims;iD++)
				{
					vClustData[j++]=vFloat[iV*iDims+iD];
				}
			}
			else if(bGetComplements)
			{
				int iD = 0;
				for(iD=0;iD<iDims;iD++)
				{
					vCompData[k++]=vFloat[iV*iDims+iD];
				}
			}
		}
		vDistribs[iC].SetData(iDims,&vClustData[0],vCounts[iC]);
		if(bGetComplements)
			vCompDistribs[iC].SetData(iDims,&vCompData[0],iCompSize);
	}
}

//this is the continuous multidimensional probability version
void FillDistribs(vector<float>& vFloat,vector<KDTreeHist>& vDistribs,vector<KDTreeHist>& vCompDistribs,int iDistribs,vector<int>& vClustIDs,vector<int>& vCounts,int iDims,A2D<int>& vBestDims,int iBestDims);
//get full 'background' distribution , containing all spikes but using dimensions specified in pBestDims
void GetFullBGDistrib(vector<float>& vFloat,KDTreeHist& oTree,int iDims,int* pBestDims,int iBestDims);

template< class T >
void FillDistribs(CVerxStack& DataStack,vector< vector<int> >& vBinData,CCluster& MainClusters,int iBins,vector< TreeHist<T> >& vDistribs,int iDistribs,vector<int>& vClustIDs)
{
	//distrib for each cluster + 1 for full distrib
	vDistribs = std::vector< TreeHist<T> >(iDistribs+1);
		
	int iV = 0;
	for(iV=0;iV<vClustIDs.size();iV++)
	{		
		//cluster spike belongs to
		vDistribs[vClustIDs[iV]].UpdateProb(&vBinData[iV],1);

		//FULL distribution containing all spikes!!
		vDistribs[iDistribs].UpdateProb(&vBinData[iV],1);
	}
}

inline void FillDistribs(CVerxStack& DataStack,vector< vector<float>* >& vFloatps,CCluster& MainClusters,int iBins,vector< TreeHist<float> >& vDistribs,int iDistribs,vector<int>& vClustIDs,int iMinD,int iMaxD)
{
	//distrib for each cluster + 1 for full distrib
	vDistribs = std::vector< TreeHist<float> >(iDistribs+1);

	int iC=1;
	for(iC=1;iC<=iDistribs;iC++)
	{
		vDistribs[iC].SetBinless(true);
		vDistribs[iC].SetDRange(iMinD,iMaxD);
	}
		
	int iV = 0;
	for(iV=0;iV<vClustIDs.size();iV++)
	{		
		//cluster spike belongs to
		vDistribs[vClustIDs[iV]].UpdateProb(vFloatps[iV],1);

		//FULL distribution containing all spikes!!
		vDistribs[iDistribs].UpdateProb(vFloatps[iV],1);
	}
}


template< class T >
void FillDistribs(CVerxStack& DataStack,CCluster& MainClusters,int iBins,vector< TreeHist<T> >& vDistribs,int iDistribs,int which,vector<vector<int> >& vBinIDs)
{
	//distrib for each cluster + 1 for full distrib
	vDistribs = vector< TreeHist<T> >(iDistribs+1);
	int iDims = DataStack.GetAutoClusteringDimension();
	
	int iV=0,iC=1; 
	
	//go through vertices
	MY_STACK::iterator Index;
	for (Index=DataStack.m_VerxStack.begin();Index!=DataStack.m_VerxStack.end();Index++,iV++)
	{	
		CVertex* verx = (CVertex*)*Index;

		//skip noise
		if(verx->GetNoise()) continue;
			
		//go through clusters filling out distrib info
		for(iC=1;iC<=iDistribs;iC++)
		{
			//either spike is in cluster or it is the FULL distribution
			//containing all spikes!!
			if(iC==iDistribs || GetVClust(verx,which)==iC)
			{
				vDistribs[iC].UpdateProb(&vBinIDs[iV],1);
			}
		}
	}
}

void FillDistribs(CVerxStack& DataStack,int** pData,CCluster& MainClusters,int iBins,std::vector< std::vector<Hist> >& vDistribs,int iDistribs,vector<int>& vClustIDs,int iMinClust=1);

bool RandAssign(CVerxStack& DataStack,CCluster& MainClusters,int iClusts,int which);

inline char GetVClust(CVertex* verx,int which)
{
	switch(which)
	{
	case CLUST_USER:
		return verx->GetClust();
		break;
	case CLUST_ORIG:
		return verx->GetOrigClust();
		break;
	case CLUST_KM:
		return verx->GetKmeansClust();
		break;
	case CLUST_INFO:
		return verx->GetInfoClust();
		break;
	case CLUST_AP:
		return verx->GetAPClust();
		break;
	case CLUST_KK:
		return verx->GetKKClust();
		break;
	case CLUST_FL:
		return verx->GetFLClust();
		break;
	}
	return 0;
}
@


1.24
log
@got rid of unused buggy code for calculating entropy, added flame clust
@
text
@d1 1
a1 1
// $Id: Hist.h,v 1.23 2008/09/16 01:46:57 samn Exp $ 
d1406 1
a1406 1
CString GetClusterInfoString(CVerxStack& MainDataStack,CCluster& MainClusters,HWND wnd=0);
@


1.23
log
@use doubles for Hist, bug fix: dont use maxentropy when calculating entropy for 1D distributions, use sumentropy!!!
@
text
@d1 1
a1 1
// $Id: Hist.h,v 1.22 2008/08/11 20:46:34 samn Exp $ 
d1356 1
a1356 1
		dSum += vWeights[i] * SumEntropy(vDistribs[i]);
d1360 1
a1360 1
inline void CalcEntropy(vector< vector< Hist> >& vDistribs,vector<hprob>& vcEntropy,int iClusts,int iDim,vector<int>& vCounts,int iNumElems,vector<int>& vZeroes,bool bWeight,bool bMax1DEntropy)
d1363 4
a1366 27
	if(vcEntropy.size() < iClusts+1) vcEntropy = vector<hprob>(iClusts+1);
	if(!bMax1DEntropy)
	{	if(bWeight)
		{	for(iC=1;iC<=iClusts;iC++)
			{	vcEntropy[iC] = ( (hprob) vCounts[iC] / (hprob) iNumElems ) * SumEntropy(vDistribs[iC],vZeroes);
			}
			if(vCounts[0]) vcEntropy[0] = -1.0 * ( (hprob) vCounts[0] / (hprob) iNumElems ) * SumEntropy(vDistribs[0],vZeroes); // ************* //
		}
		else
		{	for(iC=1;iC<=iClusts;iC++)
			{	vcEntropy[iC] = SumEntropy(vDistribs[iC],vZeroes);
			}
		}
	}
	else
	{	if(bWeight)
		{	//for(iC=1;iC<=iClusts;iC++)
			for(iC=1;iC<=iClusts;iC++) // **************** //
			{	vcEntropy[iC] = ( (hprob) vCounts[iC] / (hprob) iNumElems ) * SumEntropy(vDistribs[iC],vZeroes) * MaxEntropy(vDistribs[iC],vZeroes);
			}
			vcEntropy[0] = -1.0 * SumEntropy(vDistribs[0],vZeroes) * MaxEntropy(vDistribs[0],vZeroes); // ************* //
		}
		else
		{	for(iC=1;iC<=iClusts;iC++)
			{	vcEntropy[iC] = SumEntropy(vDistribs[iC],vZeroes) * MaxEntropy(vDistribs[iC],vZeroes);
			}
		}
d1368 1
d1565 3
@


1.22
log
@added hashmap stuff and GetFullBGDistrib
@
text
@d1 1
a1 1
// $Id: Hist.h,v 1.21 2008/07/24 18:47:32 samn Exp $ 
d1082 1
d1091 1
a1091 1
	std::vector<prob_t> m_probs;
d1094 1
a1094 1
	prob_t m_dMin;
d1097 1
a1097 1
	prob_t m_dMax;
d1100 1
a1100 1
	prob_t m_dRange;
d1106 1
a1106 1
	prob_t m_dNumElems;
d1112 2
a1113 2
	hash_map< int* ,prob_t, hash_compare<const int*, eqp> > m_cache;
	hash_map< int* ,prob_t, hash_compare<const int*, eqp> >::iterator m_IT;
d1118 1
a1118 1
	inline int ValIndex(prob_t dVal) const
d1123 1
a1123 1
		prob_t dFctr = (dVal - m_dMin) / m_dRange;
d1130 3
a1132 3
	inline prob_t Max() const { return m_dMax; }
	inline prob_t Min() const { return m_dMin; }
	inline prob_t Range() const { return m_dRange; }
d1144 4
a1147 4
		if(idx < 0 || idx >= m_counts.size())
		{
			return false;
		}
d1149 1
a1149 1
		m_bNeedReCalc = true;
d1157 4
a1160 4
		if(idx < 0 || idx >= m_counts.size())
		{
			return false;
		}
d1162 1
a1162 1
		m_bNeedReCalc = true;
d1168 1
a1168 1
	inline bool IncBinVal(prob_t dVal)
d1181 1
a1181 1
	inline bool DecBinVal(prob_t dVal)
d1194 1
a1194 1
	inline bool SetBinVal(prob_t dVal,int iCount)
d1210 1
a1210 1
	inline int GetBinVal(prob_t dVal) const { return m_counts[ValIndex(dVal)]; }
d1213 1
a1213 1
	inline prob_t GetBinProb(prob_t dVal) const{ return (prob_t) GetBinVal(dVal) / m_dNumElems; }
d1222 2
a1223 2
			m_probs = vector<prob_t>((int)m_dNumElems+1);
			prob_t j;
d1230 1
a1230 1
	inline prob_t BinProb(int i)
d1235 1
a1235 1
	inline prob_t CachedEntropy()
d1238 2
a1239 2
		prob_t dEntropy = 0.0;
		prob_t dC;
d1247 1
a1247 1
			{	prob_t dProb = m_counts[i] / m_dNumElems;
d1257 1
a1257 1
	inline prob_t Entropy()
d1260 1
a1260 1
		prob_t dEntropy = 0.0;
d1264 1
a1264 1
			prob_t dProb = BinProb(i);
d1274 1
a1274 1
	bool Init(prob_t dMin,prob_t dMax,int iBins);
d1277 1
a1277 1
	Hist(prob_t dMin,prob_t dMax,int iBins);
d1327 1
a1327 1
inline prob_t SumEntropy(vector< Hist >& vHist, vector<int>& vZero)
d1331 1
a1331 1
	prob_t dSum = 0.0;
d1340 1
a1340 1
inline prob_t SumEntropy(vector< Hist> & vHist)
d1344 1
a1344 1
	prob_t dSum = 0.0;
d1350 1
a1350 1
inline prob_t SumEntropy(vector< vector<Hist> >& vDistribs,vector<prob_t>& vWeights)
d1354 1
a1354 1
	prob_t dSum = 0.0;
d1360 1
a1360 1
inline void CalcEntropy(vector< vector< Hist> >& vDistribs,vector<prob_t>& vcEntropy,int iClusts,int iDim,vector<int>& vCounts,int iNumElems,vector<int>& vZeroes,bool bWeight,bool bMax1DEntropy)
d1363 2
a1364 2
	if(vcEntropy.size() < iClusts+1) vcEntropy = vector<prob_t>(iClusts+1);
	if(bMax1DEntropy)
d1367 1
a1367 1
			{	vcEntropy[iC] = ( (prob_t) vCounts[iC] / (prob_t) iNumElems ) * SumEntropy(vDistribs[iC],vZeroes);
d1369 1
d1379 3
a1381 2
		{	for(iC=1;iC<=iClusts;iC++)
			{	vcEntropy[iC] = ( (prob_t) vCounts[iC] / (prob_t) iNumElems ) * SumEntropy(vDistribs[iC],vZeroes) * MaxEntropy(vDistribs[iC],vZeroes);
d1383 1
d1561 1
a1561 1
void FillDistribs(CVerxStack& DataStack,int** pData,CCluster& MainClusters,int iBins,std::vector< std::vector<Hist> >& vDistribs,int iDistribs,vector<int>& vClustIDs);
@


1.21
log
@dont use lookup tables via Prob, instead just do a division with doubles
@
text
@d1 1
a1 1
// $Id: Hist.h,v 1.20 2008/07/15 03:48:38 samn Exp $ 
d1068 8
a1075 1

d1111 3
d1234 22
d1482 2
@


1.20
log
@added funcs for max entropy, not too useful
@
text
@d1 1
a1 1
// $Id: Hist.h,v 1.19 2008/06/03 16:12:01 samn Exp $ 
d889 1
a889 1
		return Prob(m_iNumElems,GetVCount(v)); 
d894 1
a894 1
		return Prob(m_iNumElems,IT->second);
d923 1
a923 1
				prob_t dProb = Prob(m_iNumElems,IT->second); //IT->second / (prob_t) m_iNumElems;
d1221 1
a1221 4
		//CalcProbs();
		//return m_probs[m_counts[i]];
		//return m_counts[i] / m_dNumElems; 
		return Prob(m_dNumElems,m_counts[i]); 
@


1.19
log
@added VHist with variable bin widths, work in progress
@
text
@d1 1
a1 1
// $Id: Hist.h,v 1.18 2008/04/22 21:33:19 samn Exp $ 
d1279 16
d1330 1
a1330 1
inline void CalcEntropy(vector< vector< Hist> >& vDistribs,vector<prob_t>& vcEntropy,int iClusts,int iDim,vector<int>& vCounts,int iNumElems,vector<int>& vZeroes,bool bWeight)
d1334 10
a1343 5
	if(bWeight)
	{
		for(iC=1;iC<=iClusts;iC++)
		{
			vcEntropy[iC] = ( (prob_t) vCounts[iC] / (prob_t) iNumElems ) * SumEntropy(vDistribs[iC],vZeroes);
d1347 9
a1355 4
	{
		for(iC=1;iC<=iClusts;iC++)
		{
			vcEntropy[iC] = SumEntropy(vDistribs[iC],vZeroes);
@


1.18
log
@added version of FillDistribs that uses best dimensions only when making vFloat multidim vector used by KDTreeHist
@
text
@d1 1
a1 1
// $Id: Hist.h,v 1.17 2008/04/14 21:46:47 samn Exp $ 
d963 106
d1071 4
@


1.17
log
@added single nearest neighbor search to avoid range searches
@
text
@d1 1
a1 1
// $Id: Hist.h,v 1.16 2008/04/11 22:28:34 samn Exp $ 
d1315 2
@


1.16
log
@added log2 version for doubles
@
text
@d1 1
a1 1
// $Id: Hist.h,v 1.15 2008/04/09 04:58:50 samn Exp $ 
d478 14
a491 1
	float GetNearestRadiusSQ(float* p,bool bAllowZeroDist)
d499 11
a509 1
		while(true)
d511 4
a514 3
			m_pTree->search_center_radius_sq(p,fRad,1);
			int iCount = m_pTree->get_count();
			if(iCount)
d516 6
a521 2
				vector<float> vRadii(iCount);
				m_pTree->copy_radii_sq(&vRadii[0]);
d523 1
a523 1
				float fm = 0.0;
d525 4
a528 4
				if(bAllowZeroDist)
					return *std::min_element(vRadii.begin(),vRadii.end());
				else 
					fm=GZeroMinElem(vRadii);
d530 5
a534 2
				if(fm>0.0)
					return fm;
d536 1
a536 3
			//increase search radius
			fRad *= two;
		}		
d539 1
a539 1
	float GetNearestRadiusSQ(int i)
d541 1
a541 1
		return GetNearestRadiusSQ(&m_vData[i*m_iDims],false);
d578 1
a578 1
		prob_t dRad = sqrt(GetNearestRadiusSQ(p,false));
d591 1
a591 1
		prob_t dRad = sqrt(GetNearestRadiusSQ(p,true));
d613 1
a613 1
			prob_t dDist = GetNearestRadiusSQ(&m_vData[iOffset],false);
@


1.15
log
@started experimenting with faster version of finding knn, added GetAllKNN -- still needs more testing!!
@
text
@d1 1
a1 1
// $Id: Hist.h,v 1.14 2008/04/07 03:42:58 samn Exp $ 
d20 6
@


1.14
log
@moved kldiv stuff into InfoT .h,.cpp , added Neighbor* version of GetKNN so as to avoid using vector<vector> (to save memory)
@
text
@d1 1
a1 1
// $Id: Hist.h,v 1.13 2008/03/17 22:15:28 samn Exp $ 
d272 1
a272 1
	float GetKNN(float* p,Neighbor* vnn,int iNNToFind,double fRadStart=8.0,double fRadFctr=2.0)
d310 85
@


1.13
log
@added ResistorAvg function that takes 2 floats
@
text
@d1 1
a1 1
// $Id: Hist.h,v 1.12 2008/03/16 21:33:09 samn Exp $ 
d272 38
a1125 45
//gets KLDiv using approximate probabilities
//P(j) = (nj+0.5)/(N+0.5jmax)
//n1+n2+...+nmax=N
//the p distribution's counts are first subtracted
//from the q distribution so that the function will
//calculate the "exclusive" information gain
//p is "partial" distribution
//q is "full" distribution
inline prob_t KLDivApproxExclusiveProb(Hist& p,Hist& q)
{
	if(p.NumBins() != q.NumBins()) return -1.0;

	prob_t d = 0.0;

	int i = 0, iBins = p.NumBins();

	prob_t eps = 1e-100;

	prob_t pelems = p.NumElems();
	prob_t qelems = q.NumElems();

	for(i=0;i<iBins;i++)
	{
		prob_t pcount = p[i];
		prob_t qcount = q[i];

		//approximate probability of bin i in distribution p
		prob_t pp = (pcount+0.5) / ( pelems + 0.5*iBins);

		//approximate probability of bin i in distribution q
		//must subtract pcount elements from numerator & denominator of probability
		//approximation so as to maintain "exclusivity" of p distribution
//		prob_t qp = (qcount - pcount + 0.5) / ( qelems - pcount + 0.5*iBins);
		prob_t qp = (qcount - pcount + 0.5) / ( qelems - pelems + 0.5*iBins);

		if(pp<=eps) continue;

		if(qp<=eps) continue;

		d += pp * ( log2(pp) - log2(qp) );
	}

	return d;
}

a1135 470

inline prob_t KLDiv(KDTreeHist& p,KDTreeHist& q)
{	
	int i=0;
#ifdef DO_TIMING
	char sMsg[1024];
	sprintf(sMsg,"KLDiv szp=%d, szq=%d, dims=%d",p.NumElems(),q.NumElems(),p.NumDims());
	ScopedTimer S(sMsg);
#endif
	
	try	//just in case something goes wrong from invalid type of data passed in
	{
		//make sure we can do calculation with div by zero and need same # of dimensions 
		//in each distribution
		if(p.NumElems() < 2 || q.NumElems()<1 || p.NumDims()!=q.NumDims()) return 0.0;

		int iLessP = 0, iLessQ = 0;

		prob_t dpq = 0.0;

		const prob_t eps = 0.0;

		int isz = p.NumElems();

		i = 0;
		for(i=0;i<isz;i++)
		{
			prob_t distp = p.GetNearestRadiusSQ(i);
			
			if(distp<=eps)
				continue;

			prob_t distq = q.GetNearestRadiusSQ(p[i],true);
			
			if(distq<=eps) 
				continue;

			dpq += log2(distq / distp) / 2.0;
		}

		dpq *= ((prob_t)p.NumDims()/p.NumElems());

		dpq += log2( (prob_t)q.NumElems() / (p.NumElems()-1.0 ) );

	#ifdef DO_TIMING
		sprintf(sMsg,"iLessP=%d iLessQ=%d",iLessP,iLessQ);
		MessageBox(0,sMsg,"WClust",MB_ICONINFORMATION);
	#endif
		
		return dpq;
	}
	catch(...)
	{
		char sMsg[1024];
		sprintf(sMsg,"KLDiv caught exception!! i=%d",i);
		MessageBox(0,sMsg,"WClust",MB_ICONERROR);
		return -666.0;
	}
}

//resistor avg.
inline prob_t KLDivSym(KDTreeHist& p,KDTreeHist& q)
{
	prob_t dpq = KLDiv(p,q), dqp = KLDiv(q,p);
	if(!dpq && !dqp) return 0.0;
	return dpq*dqp / (dpq+dqp);
}

template< class T >
inline prob_t KLDiv(TreeHist<T>& p,TreeHist<T>& q)
{
	prob_t d = 0.0;

	int i = 0;

	prob_t eps = 1e-40;

	typename std::map< vpwrap<T>, int>::iterator PIT = p.Begin();
	for(;PIT!=p.End();PIT++)
	{
		prob_t pp = p.GetITProb(PIT);
		if(pp<=eps) 
			continue;
		
		prob_t qp = q.GetVProb(PIT->first.p_);
		
		if(qp<=eps) 
			continue;
		
		d += pp * ( log2(pp) - log2(qp) );
	}
	return d;
}

//returns -1 iff num bins not equal
//otherwise returns KL divergence of histogram/distributions
//this returns "divergence" of distribution q from p
//so q should be a cluster's distribution and p should be full distribution
//make sure when passing in...
//the order matters because it is not symmetric
//however...
//In Bayesian statistics the KL divergence can be used as a measure of the information
//gain in moving from a prior distribution to a posterior distribution. If some new fact
//Y=y is discovered, it can be used to update the probability distribution for X from p(x|I)
//to a new posterior probability distribution p(x|y) using Bayes' theorem
// DKL(p(x|y)||p(x|I)) = sum( p(x|y) * log( p(x|y) / p(x|I) )
//so p can be cluster distribution and q can be full distribution
inline prob_t KLDiv(Hist& p,Hist& q)
{
	if(p.NumBins() != q.NumBins())
	{
		return -1.0;
	}

	prob_t d = 0.0;

	int i = 0 , iBins = p.NumBins();

	prob_t eps = 1e-100;

	for(i=0;i<iBins;i++)
	{
		prob_t pp = p.BinProb(i); //probability of bin i in distribution p
		
		if(pp<=eps) 
			continue;
		
		prob_t qp = q.BinProb(i); //probability of bin i in distribution q
		
		if(qp<=eps) 
			continue;
		
		d += pp * ( log2(pp) - log2(qp) );
	}

/*#ifdef _DEBUG
	if(d<0.0)
	{
		prob_t pf = FullProb(p);
		prob_t qf = FullProb(q);
		FILE* fp = fopen("kl_prob.txt","w");
		fprintf(fp,"p:\n");
		p.Print(fp);
		fprintf(fp,"q:\n");
		q.Print(fp);
		fclose(fp);
		int moo=0;
	}
#endif*/

	return d;
}

inline prob_t ResistorAvg(prob_t& p,prob_t& q)
{	if(!p && !q) return 0.0f;
	return p*q/(p+q);
}

//symmetrized form of KL-divergence
inline prob_t ResistorAvg(Hist& p,Hist& q)
{
	prob_t d12 = KLDiv(p,q), d21 = KLDiv(q,p);
	return (d12 * d21) / (d12 + d21);
}

//p is cluster distrib, q is full distrib
inline prob_t Uniqueness(Hist& p,Hist& q)
{
	return KLDiv(p,q);
}

//vZeroes = vector of dimensions that shouldn't contribute to calculations
//vZeroes[i] == 1 , iff you want to exclude dimension i
inline void CalcUDDist(vector< vector<Hist> >& vDistribs,int iClusts,vector<prob_t>& vcInf,vector< vector<prob_t> >& vcInfInter,vector<int>& vCounts,int iElems,vector<int>& vZeroes,bool bUseCounts)
{
	if(vDistribs.size() != iClusts + 2) return;

	vcInf = vector<prob_t>(iClusts+1);

	vcInfInter = vector< vector<prob_t> >(iClusts+1);
	int iC=1;
	for(iC=1;iC<=iClusts;iC++) vcInfInter[iC] = vector<prob_t>(iClusts+1);

	int iDims = vDistribs[1].size() , iD = 0;

	//uniqueness from full distribution for each cluster
	int iC1=1,iC2=1;
	for(iC1=1;iC1<=iClusts;iC1++)
	{
		prob_t kldiv=0.0;
		for(iD=0;iD<iDims;iD++)
		{
			if(vZeroes[iD]) continue;

			kldiv += KLDiv(vDistribs[iC1][iD],vDistribs[iClusts+1][iD]);
		}
		vcInf[iC1] = kldiv;
	}

	//inter-cluster distinction measures, KL divergence between
	//a cluster and cluster+other_cluster
	for(iC1=1;iC1<=iClusts;iC1++)
	{
		for(iC2=1;iC2<=iClusts;iC2++)
		{
			if(iC1==iC2)
				vcInfInter[iC1][iC2]=0.0;
			else
			{
				for(iD=0;iD<iDims;iD++)
				{
					if(vZeroes[iD]) continue;

					Hist oTmp = vDistribs[iC1][iD];
					oTmp.Add(vDistribs[iC2][iD]);
					vcInfInter[iC1][iC2] += KLDiv(vDistribs[iC1][iD],oTmp);
				}
			}				
		}
	}
	//add smallest inter-cluster KL-div
	for(iC1=1;iC1<=iClusts;iC1++)
	{
		prob_t dMinInter = 9e10;
		bool bFound = false;
		if(vCounts[iC1])
		for(iC2=1;iC2<=iClusts;iC2++)
		{
			if(iC1 != iC2 && vCounts[iC2] && vcInfInter[iC1][iC2] < dMinInter)
			{
				dMinInter = vcInfInter[iC1][iC2];
				bFound = true;
			}
		}
		if(bFound)
			vcInf[iC1] += dMinInter;
	}

	if(bUseCounts)
	{
		for(iC1=1;iC1<=iClusts;iC1++)
		{
			vcInf[iC1] = ( (prob_t)vCounts[iC1] / iElems) * vcInf[iC1];
		}
	}
}

//vZeroes = vector of dimensions that shouldn't contribute to calculations
//vZeroes[i] == 1 , iff you want to exclude dimension i
template< class T >
inline void CalcUDDist(vector< TreeHist<T> >& vDistribs,int iClusts,vector<prob_t>& vcInf,vector< vector<prob_t> >& vcInfInter,vector<int>& vCounts,int iElems,vector<int>& vZeroes,bool bUseCounts)
{
	if(vDistribs.size() != iClusts + 2) return;

	vcInf = vector<prob_t>(iClusts+1);

	vcInfInter = vector< vector<prob_t> >(iClusts+1);
	int iC=1;
	for(iC=1;iC<=iClusts;iC++) vcInfInter[iC] = vector<prob_t>(iClusts+1);

	//uniqueness from full distribution for each cluster
	int iC1=1,iC2=1;
	for(iC1=1;iC1<=iClusts;iC1++)
		vcInf[iC1] = KLDiv(vDistribs[iC1],vDistribs[iClusts+1]);

	//inter-cluster distinction measures, KL divergence between
	//a cluster and cluster+other_cluster
	for(iC1=1;iC1<=iClusts;iC1++)
	{
		for(iC2=1;iC2<=iClusts;iC2++)
		{
			if(iC1==iC2)
				vcInfInter[iC1][iC2]=0.0;
			else
			{
				TreeHist<T> oTmp = vDistribs[iC1];
				oTmp.Add(vDistribs[iC2]);
				vcInfInter[iC1][iC2] += KLDiv(vDistribs[iC1],oTmp);
			}				
		}
	}
	//add smallest inter-cluster KL-div
	for(iC1=1;iC1<=iClusts;iC1++)
	{
		prob_t dMinInter = 9e10;
		bool bFound = false;
		if(vCounts[iC1])
		for(iC2=1;iC2<=iClusts;iC2++)
		{
			if(iC1 != iC2 && vCounts[iC2] && vcInfInter[iC1][iC2] < dMinInter)
			{
				dMinInter = vcInfInter[iC1][iC2];
				bFound = true;
			}
		}
		if(bFound)
			vcInf[iC1] += dMinInter;
	}

	if(bUseCounts)
	{
		for(iC1=1;iC1<=iClusts;iC1++)
		{
			vcInf[iC1] = ( (prob_t)vCounts[iC1] / iElems) * vcInf[iC1];
		}
	}
}

inline void CalcUDDist(vector< KDTreeHist >& vDistribs,vector< KDTreeHist >& vCompDistribs,int iClusts,vector<prob_t>& vcInf,vector< vector<prob_t> >& vcInfInter,vector<int>& vCounts)
{
	if(vDistribs.size() != iClusts + 2 || vDistribs.size()!=vCompDistribs.size()) return;

	vcInf = vector<prob_t>(iClusts+1);

	vcInfInter = vector< vector<prob_t> >(iClusts+1);
	int iC=1;
	for(iC=1;iC<=iClusts;iC++) vcInfInter[iC] = vector<prob_t>(iClusts+1);

	//uniqueness from full distribution for each cluster
	int iC1=1,iC2=1;
	for(iC1=1;iC1<=iClusts;iC1++)
		vcInf[iC1] = KLDiv(vDistribs[iC1],vCompDistribs[iC1]);//KLDivSym(vDistribs[iC1],vCompDistribs[iC1]);

	//inter-cluster distinction measures, KL divergence between
	//a cluster and cluster+other_cluster
	for(iC1=1;iC1<=iClusts;iC1++)
	{
		//for(iC2=iC1+1;iC2<=iClusts;iC2++)
		for(iC1=1;iC2<=iClusts;iC2++)
		{
			if(iC1==iC2)
				vcInfInter[iC1][iC2]=0.0;
			else
				//vcInfInter[iC1][iC2] =  vcInfInter[iC2][iC1] = KLDivSym(vDistribs[iC1],vDistribs[iC2]);
				vcInfInter[iC1][iC2] =  KLDiv(vDistribs[iC1],vDistribs[iC2]);
		}
	}
	//add smallest inter-cluster KL-div
	for(iC1=1;iC1<=iClusts;iC1++)
	{
		prob_t dMinInter = 9e10;
		bool bFound = false;
		if(vCounts[iC1])
		for(iC2=1;iC2<=iClusts;iC2++)
		{
			if(iC1 != iC2 && vCounts[iC2] && vcInfInter[iC1][iC2] < dMinInter)
			{
				dMinInter = vcInfInter[iC1][iC2];
				bFound = true;
			}
		}
		if(bFound)
			vcInf[iC1] += dMinInter;
	}
}

//calculates information gain as resistor average of all clusters against full distribution
//plus smallest resistor average between cluster and all other clusters
//vDistribs must have size of iClusts + 1
inline void CalcRDist(vector< vector<Hist> >& vDistribs,int iClusts,vector<prob_t>& vcInf,vector< vector<prob_t> >& vcInfInter)
{
	if(vDistribs.size() != iClusts + 2) return;

	vcInf = vector<prob_t>(iClusts+1);

	vcInfInter = vector< vector<prob_t> >(iClusts+1);
	int iC=1;
	for(iC=0;iC<=iClusts;iC++) vcInfInter[iC] = vector<prob_t>(iClusts+1);

	int iDims = vDistribs[1].size() , iD = 0;

	//only need this for first time, since its recalculated for distributions who's
	//points change, when they change
	int iC1=1,iC2=1;
	for(iC1=1;iC1<=iClusts;iC1++)
	{
		prob_t kldiv=0.0;
		for(iD=0;iD<iDims;iD++)
		{
			kldiv += ResistorAvg(vDistribs[iC1][iD],vDistribs[iClusts+1][iD]);
		}
		vcInf[iC1]=kldiv;
	}

	//fill inter-cluster resistor averages
	for(iC1=1;iC1<=iClusts;iC1++)
	{
		for(iC2=1;iC2<=iC1;iC2++)
		{
			if(iC1==iC2)
				vcInfInter[iC1][iC2]=0.0;
			else
			{
				for(iD=0;iD<iDims;iD++)
				{
					vcInfInter[iC1][iC2]+=ResistorAvg(vDistribs[iC1][iD],vDistribs[iC2][iD]);
				}
				vcInfInter[iC2][iC1]=vcInfInter[iC1][iC2];
			}				
		}
	}
	//add largest inter-cluster resistor average
	for(iC1=1;iC1<=iClusts;iC1++)
	{
		prob_t dMinInter = 9e10;
		bool bFound = false;
		for(iC2=1;iC2<=iClusts;iC2++)
		{
			if(iC1 != iC2 && vcInfInter[iC1][iC2] < dMinInter)
			{
				dMinInter = vcInfInter[iC1][iC2];
				bFound = true;
			}
		}
		if(bFound)
			vcInf[iC1] += dMinInter;
	}
}

template< class T > 
T Sum(vector<T>& v)
{
	int i;
	T val = T(0);
	for(i=0;i<v.size();i++) val += v[i];
	return val;
}

template< class T > 
T Sum(vector<T>& v,int iStart,int iEnd)
{
	int i;
	T val = T(0);
	for(i=iStart;i<v.size() && i<iEnd;i++) val += v[i];
	return val;
}

template< class T > 
T Avg(vector<T>& v)
{
	if(!v.size()) return T(0);
	int i;
	T val = T(0);
	for(i=0;i<v.size();i++) val += v[i];
	return val / (T) v.size();
}

template< class T > 
T Avg(vector<T>& v,int iStart,int iEnd)
{
	if(!v.size()) return T(0);
	int i;
	T val = T(0);
	for(i=iStart;i<iEnd;i++) val += v[i];
	return val / (T) (iEnd-iStart);
}

template< class T >
T Variance(vector<T>& v,int iStart,int iEnd)
{	T avg = Avg(v,iStart,iEnd);
	int i;
	T var = T(0) , val = T(0);
	for(i=iStart;i<iEnd;i++)
	{	val = v[i]-avg;
		var += val*val;
	}
	var /= (T)(iEnd-iStart);
	return var;
}

@


1.12
log
@added optimize code which has max # of neighbors to find in kdtree (which is commented out for now)
@
text
@d1 1773
a1773 1768
// $Id: Hist.h,v 1.11 2008/03/05 04:46:49 samn Exp $ 
#pragma once
#include "Cluster.h"
#include "WCMath.h"
#include "KDTree.h"
#include "ScopedTimer.h"
#include "Log.h"
#include <map>
#include <set>

//#define DO_TIMING

//binary logarithm
inline prob_t log2(prob_t d)
{
	static prob_t dl2 = log(2.0);
	return log(d) / dl2;
}

prob_t Prob(int,int);

void InitProbs(int iMaxNumElems);

struct ProbInitFree
{
	ProbInitFree(int i);
	~ProbInitFree();
};

inline bool GZeroMinElem(float f1,float f2)
{
	return f1 > 0.0 && (f1 < f2 || f2 <= 0.0);
}

inline float GZeroMinElem(vector<float>& v)
{
	float fm = *std::max_element(v.begin(),v.end());
	int i=0,sz=v.size();
	for(;i<sz;i++)
		if(v[i]<fm && v[i]>0.0)
			fm=v[i];
	return fm;
}

inline float MinElem(vector<float>& v,bool bAllowZero)
{
	float fm = *std::max_element(v.begin(),v.end());
	int i=0,sz=v.size();
	for(;i<sz;i++)
		if(v[i]<fm && (bAllowZero || v[i]>0.0))
			fm=v[i];
	return fm;
}

inline int MinIdx(vector<float>& v,bool bAllowZero)
{
	float fm = *std::max_element(v.begin(),v.end());
	int i=0,sz=v.size(), minID = 0;
	bool bFound = false;
	for(;i<sz;i++)
	{
		if(v[i]<fm && (bAllowZero || v[i]>0.0))
		{
			bFound = true;
			fm=v[i];
			minID=i;
		}
	}
	if(!bFound) return -1;
	return minID;
}

struct Neighbor
{
	prob_t m_dist;
	int m_id;
	Neighbor(int id,prob_t dist)
		:m_id(id),m_dist(dist){}
	Neighbor()
		:m_id(0),m_dist(0){}
};

inline bool operator<(Neighbor& n1,Neighbor& n2)
{
	return n1.m_dist<n2.m_dist;
}

//kd tree from biopython used for continuous probability distribution estimates 
class KDTreeHist
{
	NSKDTree::KDTree* m_pTree;

	int m_iDims;
	int m_iNumElems;	
	prob_t m_dPiPow;
	prob_t m_dTop;
	prob_t m_dGamma;

	vector<float> m_vData;
	
	vector<float> m_vProbs;

public:
	
	KDTreeHist()
		:m_iDims(0),
		 m_iNumElems(0),
		 m_pTree(0)
	{
	}

	virtual ~KDTreeHist()
	{
		if(m_pTree) delete m_pTree;
	}

	prob_t Top(){ return m_dTop; }
	prob_t PiPow(){ return m_dPiPow; }
	prob_t Gam(){ return m_dGamma; }

	bool SetData(vector<float>& vFloat,vector<int>& vIDs,int iNumPoints,int iDims,int iCID,bool bNot=false)
	{
		if(!vFloat.size() || iNumPoints<1) return false;

		int iV = 0 , iC = 0;

		int iTotalVs = vFloat.size() / iDims;

		m_vData = vector<float>(iNumPoints*iDims);
		int j = 0 , k = 0;
		if(bNot)
		{
			for(iV=0;iV<vIDs.size();iV++)
			{
				if(vIDs[iV] != iCID && vIDs[iV]<1000)
				{
					int iD = 0;
					for(iD=0;iD<iDims;iD++)
					{
						m_vData[j++]=vFloat[iV*iDims+iD];
					}
				}
			}
		}
		else
		{
			for(iV=0;iV<vIDs.size();iV++)
			{
				if(vIDs[iV] == iCID)
				{
					int iD = 0;
					for(iD=0;iD<iDims;iD++)
					{
						m_vData[j++]=vFloat[iV*iDims+iD];
					}
				}
			}
		}

		m_iDims = iDims;
		
		if(m_pTree) delete m_pTree;
		
		m_pTree = new NSKDTree::KDTree(m_iDims,8,false);
		m_iNumElems = iNumPoints;

		const prob_t PI=3.14159265358979323846;

		m_dPiPow = pow((prob_t)PI,(prob_t)m_iDims/two);
		m_dTop = (one/(m_iNumElems-one));
		m_dGamma = Gamma(m_iDims/two+one);
		
		m_pTree->set_data(&m_vData[0],iNumPoints);

		return true;
	}

	bool SetData(int iDims,float* pData,int iNumPoints)
	{
		if(!pData || iNumPoints<1) return false;

		m_iDims = iDims;
		if(m_pTree) delete m_pTree;
		//m_pTree = new NSKDTree::KDTree(m_iDims,16,false);
		m_pTree = new NSKDTree::KDTree(m_iDims,8,false);
		m_iNumElems = iNumPoints;

		m_vData = vector<float>(iNumPoints*m_iDims);
		memcpy(&m_vData[0],pData,iNumPoints*m_iDims*sizeof(float));

		const prob_t PI=3.14159265358979323846;

		m_dPiPow = pow((prob_t)PI,(prob_t)m_iDims/two);
		m_dTop = (one/(m_iNumElems-one));
		m_dGamma = Gamma(m_iDims/two+one);
		
		m_pTree->set_data(&m_vData[0],iNumPoints);

	//	m_vProbs = vector<float>(iNumPoints);
		
		return true;
	}

	int NumElems()
	{
		return m_iNumElems;
	}

	int NumDims()
	{
		return m_iDims;
	}

	float* operator[](int i)
	{
		if(i >= m_iNumElems) return 0;
		
		return &m_vData[i*m_iDims];
	}

	float GetKNN(float* p,vector<Neighbor>& vnn,int iNNToFind,double fRadStart=8.0,double fRadFctr=2.0)
	{
		if(m_iNumElems == 1) return 0.0;
		
		int iIter = 0;
		float fRad = fRadStart;

		while(true)
		{
			m_pTree->search_center_radius_sq(p,fRad,iNNToFind);
			int iCount = m_pTree->get_count();
			if(iCount>iNNToFind)// || (iCount&&iNNToFind==1))
			{
				vector<float> vRadii(iCount);
				m_pTree->copy_radii_sq(&vRadii[0]);

				vector<long> vID(iCount);
				m_pTree->copy_indices(&vID[0]);

				vnn=vector<Neighbor>(iCount);

#ifdef _DEBUG
				Write2Log("found %d neighbors want %d",iCount,iNNToFind);
#endif

				int i = 0, j = 0;
				for(i=0;i<iCount;i++)
					if(vRadii[i]>0.0)
						vnn[j++] = Neighbor(vID[i],vRadii[i]);
				
				vnn.resize(j);
				std::sort(vnn.begin(),vnn.end());

				if(vnn.size()>iNNToFind)
				{
					vector<Neighbor> vNNTmp(iNNToFind);
					std::copy(vnn.begin(),vnn.begin()+iNNToFind,vNNTmp.begin());
					vnn=vNNTmp;
				}

#ifdef _DEBUG
				prob_t ttt = vnn[0].m_dist;
#endif
				
				return fRad;
			}
			//increase search radius
			fRad *= fRadFctr;
		}		
	}

	//get nearest neighbor as float*
	float* GetNearestNeighbor(float* p,bool bAllowZeroDist)
	{
		if(m_iNumElems == 1) return 0;
		
		const int iNumRads = 7;
		float pRads[7] = {3.0f,30.0f,150.0f,300.0f,600.0f,900.0f,1000.0f};

		int iIter = 0;
		float fRad = pRads[0];

		while(true)
		{
			m_pTree->search_center_radius_sq(p,fRad,1);
			int iCount = m_pTree->get_count();
			if(iCount)
			{
				vector<float> vRadii(iCount);
				m_pTree->copy_radii_sq(&vRadii[0]);

				int id = MinIdx(vRadii,bAllowZeroDist);
				if(id != -1)
				{
					vector<long> vID(iCount);
					m_pTree->copy_indices(&vID[0]);
					return &m_vData[vID[id]*m_iDims];
				}
			}
			//increase search radius
			if(iIter+1 >= iNumRads)
				fRad *= two;
			else
				fRad = pRads[++iIter];
		}		
	}

	float GetNearestRadiusSQ(float* p,vector<int>& vMap,int iID)
	{
		if(m_iNumElems == 1) return 0.0;
		
		int iIter = 0;
		extern prob_t gstartrad;
		float fRad = gstartrad;

		while(true)
		{
			m_pTree->search_center_radius_sq(p,fRad,1);
			int iCount = m_pTree->get_count();
			if(iCount)
			{
				vector<float> vRadii(iCount);
				m_pTree->copy_radii_sq(&vRadii[0]);

				float fm = FLT_MAX; 

				vector<long> vIndices(iCount);
				m_pTree->copy_indices(&vIndices[0]);

				bool bFound = false;

				int i = 0;
				for(i=0;i<iCount;i++)
				{
					if(vMap[vIndices[i]]==iID && vRadii[i]<= fm && vRadii[i]>0.0)
					{	bFound = true;
						fm = vRadii[i];
					}
				}

				if(bFound)
					return fm;
			}
			//increase search radius
			fRad *= two;
		}		
	}

	float GetNearestRadiusSQ(float* p,bool bAllowZeroDist)
	{
		if(m_iNumElems == 1) return 0.0;		

		int iIter = 0;
		extern prob_t gstartrad;
		float fRad = gstartrad;

		while(true)
		{
			m_pTree->search_center_radius_sq(p,fRad,1);
			int iCount = m_pTree->get_count();
			if(iCount)
			{
				vector<float> vRadii(iCount);
				m_pTree->copy_radii_sq(&vRadii[0]);

				float fm = 0.0;

				if(bAllowZeroDist)
					return *std::min_element(vRadii.begin(),vRadii.end());
				else 
					fm=GZeroMinElem(vRadii);

				if(fm>0.0)
					return fm;
			}
			//increase search radius
			fRad *= two;
		}		
	}

	float GetNearestRadiusSQ(int i)
	{
		return GetNearestRadiusSQ(&m_vData[i*m_iDims],false);
	}

	//returns probability based on distance
	//of an arbitrary element in THIS distribution
	//to it's nearest neighbor in THIS distribution
	prob_t RProb(prob_t dRad)
	{
		return m_dTop / (m_dPiPow*dRad*m_dGamma);
	}

	//returns probability based on distance
	//of an arbitrary element in a DIFFERENT distribution
	//to it's nearest neighbor in THIS distribution
	prob_t RProbOther(prob_t dRad)
	{
		return (one/m_iNumElems) / (m_dPiPow*dRad*m_dGamma);
	}

	//returns probability of element i
	prob_t IProb(int i)
	{
		if(!m_pTree || i<0 || i>=m_iNumElems) return 0.0;

		if(1==m_iNumElems)return 1.0;

		return VProb(&m_vData[i*m_iDims]);
	}

	//returns probability of vector p
	//vector p must be in THIS distribution
	prob_t VProb(float* p)
	{
		if(!p || !m_pTree) return 0.0;

		if(1==m_iNumElems)return 1.0;

		prob_t dRad = sqrt(GetNearestRadiusSQ(p,false));

		return RProb(dRad);
	}

	//returns probability of vector p
	//vector p must be in DIFFERENT distribution
	prob_t VProbOther(float* p)
	{
		if(!p || !m_pTree) return 0.0;

		if(1==m_iNumElems)return 1.0;

		prob_t dRad = sqrt(GetNearestRadiusSQ(p,true));

		if(dRad == 0.0) return 0.0;

		return RProbOther(dRad);
	}

	char m_strMsg[1024];

	//entropy of distribution
	prob_t Entropy()
	{	//sprintf(m_strMsg,"Entropy sz=%d",m_iNumElems);
		//ScopedTimer S(m_strMsg);
		if(m_iNumElems<2) return 0.0;
		
		prob_t dEntrop = 0.0;
		prob_t dPiPowGamma = m_dPiPow*m_dGamma;

		int isz = m_iNumElems , i=0, iOffset = 0;
		
		for(i=0;i<isz;i++)
		{
			prob_t dDist = GetNearestRadiusSQ(&m_vData[iOffset],false);
			if(dDist<=0.0)continue;
			dDist = sqrt(dDist); 
			prob_t dProb = m_dTop / (dDist*dPiPowGamma);
			if(dProb<=0.0)continue;
			dEntrop += dProb * log2(dProb);
			iOffset += m_iDims;
		}
		return -dEntrop;
	}

	//vIDs specifies which cluster each element
	//belongs to. iClust specifies which cluster
	//to get entropy for
	prob_t Entropy(vector<int>& vIDs,int iClust)
	{
		prob_t dEntrop = 0.0;
		int isz = m_iNumElems , i=0;
		for(i=0;i<isz;i++)
		{
			if(vIDs[i]==iClust)
			{
				prob_t dProb = IProb(i);
				if(dProb==0.0) continue;
				dEntrop += dProb * log2(dProb);
			}
		}
		return -dEntrop;
	}

	//vIDs specifies which cluster each element
	//belongs to. iClust specifies which cluster
	//to get entropy for
	prob_t Entropy(vector<int>& vIDs,int iClust,int iNumElems)
	{	sprintf(m_strMsg,"Entropy c%d sz=%d totsz=%d",iClust,iNumElems,m_iNumElems);
		ScopedTimer S(m_strMsg);
		if(iNumElems<2) return 0.0;

		const prob_t PI=3.14159265358979323846;
		prob_t dPiPow = pow((prob_t)PI,(prob_t)m_iDims/two);
		prob_t dTop = (one/(iNumElems-one));
		prob_t dGamma = Gamma(m_iDims/two+one);
		prob_t dPiPowGamma = dPiPow*dGamma;

		prob_t dEntrop = 0.0;
		int isz = m_iNumElems , i=0;
		for(i=0;i<isz;i++)
		{
			if(vIDs[i]==iClust)
			{
				prob_t dDist = GetNearestRadiusSQ(&m_vData[i*m_iDims],vIDs,iClust);
				if(dDist<=0.0)continue;
				dDist=sqrt(dDist);
				prob_t dProb = dTop / (dDist*dPiPowGamma);
				if(dProb<=0.0)continue;
				dEntrop += dProb * log2(dProb);
			}
		}
		return -dEntrop;
	}
};

template< class T >
struct vpwrap
{
	vector<T>* p_;
	int iMax_, iMin_;
	
	vpwrap(vector<T>* p,int iMin=-1,int iMax=-1)
		:p_(p),
	     iMax_(iMax),
		 iMin_(iMin){}

	//global operator== is used, not this one
    //this one doesn't even get compiled
	bool operator==(const vpwrap<T>& vr) const
	{
		if(iMax_ == -1 || iMin_ == -1) 
			return *p_ == *vr.p_;
		
		dont
		
		vector<T>& v1 = *p_;
		vector<T>& v2 = *vr.p_;
		
		int i = iMin_;
		for(;i<iMax_;i++)
			if(v1[i] != v2[i]) 
				return false;
		
		return true;
	}

	bool operator<(const vpwrap<T>& vr) const
	{
		if(iMax_ == -1 || iMin_ == -1) 
			return *p_ < *vr.p_;

		vector<T>& v1 = *p_;
		vector<T>& v2 = *vr.p_;

		return lexicographical_compare(&v1[iMin_],&v1[iMax_],&v2[iMin_],&v2[iMax_]);
	}

	T Dist(const vpwrap<T>& vr, T tMinSoFar) const
	{
		vector<T>& v1 = *p_;
		vector<T>& v2 = *vr.p_;

		int iMinT = iMin_ == -1 ? 0 : iMin_,
			iMaxT = iMax_ == -1 ? v1.size() : iMax_;

		T tDist(0);
		
		int i = iMinT;

		for(;i<iMaxT;i++)
		{
			T val = v1[i] - v2[i];
			val *= val;
			tDist += val;
			if(tDist > tMinSoFar) return tDist;
		}
		
		return tDist;
	}

	void Print(FILE* fp=stdout,bool bNewLine=false) const
	{
		vector<T>& v = *p_;		
		
		int iMinT = iMin_ == -1 ? 0 : iMin_;
		int iMaxT = iMax_ == -1 ? v.size() : iMax_;
		int i = iMinT;
		
		if(sizeof(T)==sizeof(int))
		{
			for(;i<iMaxT;i++)
			{
				fprintf(fp," %d ",v[i]);
			}
		}
		else// if(sizeof(T)==sizeof(float))
		{
			for(;i<iMaxT;i++)
			{
				fprintf(fp," %f ",v[i]);
			}
		}
		if(bNewLine) fprintf(fp,"\n");
	}
};

inline bool operator==(const vpwrap<float>& vl,const vpwrap<float>& vr)
{
	if(vl.iMax_ == -1 || vl.iMin_ == -1) 
		return *vl.p_ == *vr.p_;
	
	vector<float>& v1 = *vl.p_;
	vector<float>& v2 = *vr.p_;
	
	int i = vl.iMin_;
	for(;i<vl.iMax_;i++)
		if(v1[i] != v2[i]) 
			return false;
	
	return true;
}

inline bool operator==(const vpwrap<int>& vl,const vpwrap<int>& vr)
{
	if(vl.iMax_ == -1 || vl.iMin_ == -1) 
		return *vl.p_ == *vr.p_;
	
	vector<int>& v1 = *vl.p_;
	vector<int>& v2 = *vr.p_;
	
	int i = vl.iMin_;
	for(;i<vl.iMax_;i++)
		if(v1[i] != v2[i]) 
			return false;
	
	return true;
}

typedef std::map< vpwrap<int> , int> THMapI;
typedef THMapI::iterator THMapITI;

typedef std::map< vpwrap<float> , int> THMapF;
typedef THMapF::iterator THMapITF;

template< class T >
class TreeHist
{
	typename std::map< vpwrap<T> , int> m_hist;
	int m_iNumDims;
	int m_iNumElems;
	bool m_bBinless;
	int m_iMinD;
	int m_iMaxD;

public:

	void SetDRange(int iMinD,int iMaxD){ m_iMinD=iMinD; m_iMaxD=iMaxD; }

	bool GetBinless(){ return m_bBinless; }
	void SetBinless(bool b){ m_bBinless=b; }
	
	typename std::map< vpwrap<T> , int>::iterator Begin(){ return m_hist.begin(); }
	typename std::map< vpwrap<T> , int>::iterator End(){ return m_hist.end(); }
	int Size(){ return m_hist.size(); }

	void Print(FILE* fp=stdout)
	{
		int i = 0;
		std::map< vpwrap<T> , int>::iterator IT = m_hist.begin();
		for(;IT!=m_hist.end();IT++)
		{
			fprintf(fp,"vec%d  ",i++);
			IT->first.Print(fp,true);
			fprintf(fp,"count = %d, probability = %.4f\n\n",IT->second,GetITProb(IT));
		}
	}

	prob_t SumProb()
	{
		if(m_iNumElems == 0) return 0.0;

		prob_t dSum = 0.0;
		std::map< vpwrap<T> , int>::iterator IT = Begin();
		for(;IT!=End();IT++)
		{
			dSum += IT->second;
		}
		dSum /= (prob_t) m_iNumElems;
		return dSum;
	}

	bool UpdateProb( vector<T>* v,int iCount)
	{
		vpwrap<T> vr(v,m_iMinD,m_iMaxD);
		std::map< vpwrap<T> , int>::iterator IT = m_hist.find(vr);
		if(IT==m_hist.end())
		{
			m_hist.insert( make_pair(vr,iCount) );
			m_iNumElems += iCount;
		}
		else
		{
#ifdef _DEBUG
			vector<T>& v1 = *v;
			vector<T>& v2 = *IT->first.p_;
			int moo=0;
#endif
			IT->second+=iCount;
			m_iNumElems += iCount;

			if(IT->second == 0)
				m_hist.erase(IT);
		}
		return true;
	}

	//get count of bin that dVal is in 
	inline int GetVCount( vector<int>* v)  
	{ 
		typename std::map< vpwrap<T> , int>::iterator IT = m_hist.find(vpwrap<T>(v,m_iMinD,m_iMaxD));
		if(IT==m_hist.end())
			return 0;
		return IT->second;
	}

	//get probability of variable being bin i
	inline prob_t GetVProb(vector<int>* v) 
	{
		if(m_iNumElems == 0) return 0.0;
		return Prob(m_iNumElems,GetVCount(v)); 
	}

	inline prob_t GetITProb(typename std::map< vpwrap<T> , int>::iterator& IT)
	{
		return Prob(m_iNumElems,IT->second);
	}

	inline prob_t Entropy()
	{
		int i = 0;
		typename std::map< vpwrap<T> , int>::iterator IT = m_hist.begin();
		prob_t dEntropy = 0.0;

		if(m_bBinless)
		{
			if(1==m_hist.size())return 0.0;
			prob_t dSz = m_hist.size();
			prob_t dDim = Begin()->first.iMax_ - Begin()->first.iMin_ + 1.0;
			prob_t dPiPow = pow(PI,(prob_t)dDim/2.0);
			prob_t dTop = (1.0/(dSz-1.0));
			prob_t dGamma = Gamma(dDim/2.0+1.0);
			for(;IT!=m_hist.end();IT++)
			{
				T tDistC = sqrt( (prob_t) ClosestDist(IT));
				if(tDistC == 0.0) continue;
				prob_t dProb = (IT->second*dTop) / (dPiPow*tDistC*dGamma);
				dEntropy += dProb * log2(dProb);
			}
		}
		else
		{
			for(;IT!=m_hist.end();IT++)
			{
				prob_t dProb = Prob(m_iNumElems,IT->second); //IT->second / (prob_t) m_iNumElems;
				if(dProb == 0.0) continue;
				dEntropy += dProb * log2(dProb);
			}
		}
		return -dEntropy;
	}
	
	TreeHist(){ m_iNumElems = 0; m_bBinless = false; m_iMinD=-1;m_iMaxD=-1;};
	~TreeHist(){};

	//add h to this
	void Add(TreeHist& h)
	{
		typename std::map< vpwrap<T> , int>::iterator IT = h.m_hist.begin();
		for(;IT!=h.m_hist.end();IT++)
		{
			UpdateProb(IT->first.p_,IT->second);
		}
	}

	T ClosestDist(typename const std::map< vpwrap<T>, int>::iterator ITOther)
	{
		T tMinSoFar = 9e10;
		
		typename std::map< vpwrap<T>, int>::iterator IT = Begin();
		
		for(;IT!=End();IT++)
		{
			if(IT == ITOther) continue;

			T tDistTmp = IT->first.Dist(ITOther->first,tMinSoFar);
			
			if(tDistTmp < tMinSoFar)
				tMinSoFar = tDistTmp;
		}
		return tMinSoFar;
	}
};

//histogram class, with automatic counting/scaling
//of values to be between min & max
class Hist
{
protected:
	
	//counts of values
	std::vector<int> m_counts;

	//vector of probability values, has probs for (0 - m_iNumElems) / m_iNumElems
	std::vector<prob_t> m_probs;
	
	//min value in histogram
	prob_t m_dMin;
	
	//max value in histogram
	prob_t m_dMax;

	//m_dMax - m_dMin
	prob_t m_dRange;
	
	//number of bins
	int m_iBins;

	//# of elements in distribution
	prob_t m_dNumElems;

	//whether need to recalc probabilities
	//(uses lazy evaluation)
	bool m_bNeedReCalc;

public:

	//return bin # for value
	inline int ValIndex(prob_t dVal) const
	{
		if(m_dRange==0.0) return 0;

		//dVal's location in range as # btwn 0 - 1
		prob_t dFctr = (dVal - m_dMin) / m_dRange;
	
		//multiply by # of bins - 1 to have it 0-based index
		//doesn't use rounding to closest bin, but probably should
		return dFctr * (m_iBins - 1);
	}
	
	inline prob_t Max() const { return m_dMax; }
	inline prob_t Min() const { return m_dMin; }
	inline prob_t Range() const { return m_dRange; }
	inline int NumBins() const { return m_iBins; }

	//number of elements in "distribution"
	inline int NumElems() const
	{
		return (int) m_dNumElems;
	}

	//increment bin by 1
	inline bool DecBin(int idx)
	{
		if(idx < 0 || idx >= m_counts.size())
		{
			return false;
		}
		m_dNumElems -= 1;
		m_bNeedReCalc = true;
		m_counts[idx] -= 1;	
		return true;
	}

	//increment bin by 1
	inline bool IncBin(int idx)
	{
		if(idx < 0 || idx >= m_counts.size())
		{
			return false;
		}
		m_dNumElems += 1;
		m_bNeedReCalc = true;
		m_counts[idx] += 1;	
		return true;
	}

	//increment bin that dVal is in by 1
	inline bool IncBinVal(prob_t dVal)
	{
		if(dVal < m_dMin || dVal > m_dMax)
		{
			return false;
		}
		m_dNumElems += 1;
		m_bNeedReCalc = true;
		m_counts[ValIndex(dVal)] += 1;	
		return true;
	}

	//decrement bin that dVal is in by 1
	inline bool DecBinVal(prob_t dVal)
	{
		if(dVal < m_dMin || dVal > m_dMax)
		{
			return false;
		}	
		m_dNumElems -= 1;
		m_bNeedReCalc=true;
		m_counts[ValIndex(dVal)] -= 1;	
		return true;
	}
	
	//set bin that dVal is in to iCount
	inline bool SetBinVal(prob_t dVal,int iCount)
	{
		if(dVal < m_dMin || dVal > m_dMax)
		{
			return false;
		}
		int iDx = ValIndex(dVal);
		int iTmp = m_counts[iDx];
		m_dNumElems -= iTmp;
		m_dNumElems += iCount;
		m_counts[iDx] = iCount;
		m_bNeedReCalc = iCount != iTmp;
		return true;
	}
	
	//get count of bin that dVal is in 
	inline int GetBinVal(prob_t dVal) const { return m_counts[ValIndex(dVal)]; }

	//get probability of variable being bin i
	inline prob_t GetBinProb(prob_t dVal) const{ return (prob_t) GetBinVal(dVal) / m_dNumElems; }

	//calc probabilities of 0 - m_dNumElems to help save time
	//only recalculate when the # of elements changes
	//not used
	inline void CalcProbs()
	{
		if(m_bNeedReCalc && m_dNumElems>=1.0)
		{
			m_probs = vector<prob_t>((int)m_dNumElems+1);
			prob_t j;
			for(j=0.0;j<=m_dNumElems;j+=1.0) m_probs[j] = j / m_dNumElems;
			m_bNeedReCalc=false;
		}
	}

	//get probability of bin i
	inline prob_t BinProb(int i)
	{ 
		//CalcProbs();
		//return m_probs[m_counts[i]];
		//return m_counts[i] / m_dNumElems; 
		return Prob(m_dNumElems,m_counts[i]); 
	}

	inline prob_t Entropy()
	{
		int i = 0;
		prob_t dEntropy = 0.0;
		for(;i<m_counts.size();i++)
		{
			if(!m_counts[i]) continue;
			prob_t dProb = BinProb(i);
			dEntropy += dProb * log2(dProb);
		}
		return -dEntropy;
	}
	
	//direct access to bin
	const int& operator[] (int i) const { return m_counts[i]; }
	
	//initialize histogram
	bool Init(prob_t dMin,prob_t dMax,int iBins);

	Hist(void);
	Hist(prob_t dMin,prob_t dMax,int iBins);
	~Hist(void);

	//add h to this
	void Add(Hist& h)
	{
		int i;
		for(i=0;i<m_iBins;i++)
		{
			m_counts[i] += h.m_counts[i];
			m_dNumElems += h.m_counts[i];
		}
	}

	inline void Print(FILE* fp)
	{
		int i;
		for(i=0;i<m_iBins;i++)
		{
			fprintf(fp,"B%d : ",i);
			int j;
			fprintf(fp,"%d p=%0.2f ",m_counts[i],BinProb(i));
			for(j=0;j<m_counts[i];j++)
			{
				fprintf(fp,"*");
			}
			fprintf(fp,"\n");
		}
		fprintf(fp,"numelems=%d\n\n",NumElems());
	}
};

//sum entropy with option of zeroing out certain dimensions
//if vZero[i] != 0, skip that dimension
inline prob_t SumEntropy(vector< Hist >& vHist, vector<int>& vZero)
{
	int iSz = vHist.size();
	int i;
	prob_t dSum = 0.0;
	for(i=0;i<iSz;i++)
	{
		if(vZero[i]) continue;
		dSum += vHist[i].Entropy();
	}
	return dSum;
}

inline prob_t SumEntropy(vector< Hist> & vHist)
{
	int iSz = vHist.size();
	int i;
	prob_t dSum = 0.0;
	for(i=0;i<iSz;i++)
		dSum += vHist[i].Entropy();
	return dSum;
}

inline prob_t SumEntropy(vector< vector<Hist> >& vDistribs,vector<prob_t>& vWeights)
{
	int iSz = vDistribs.size();
	int i;
	prob_t dSum = 0.0;
	for(i=0;i<iSz;i++)
		dSum += vWeights[i] * SumEntropy(vDistribs[i]);
	return dSum;
}

inline void CalcEntropy(vector< vector< Hist> >& vDistribs,vector<prob_t>& vcEntropy,int iClusts,int iDim,vector<int>& vCounts,int iNumElems,vector<int>& vZeroes,bool bWeight)
{
	int iC;
	if(vcEntropy.size() < iClusts+1) vcEntropy = vector<prob_t>(iClusts+1);
	if(bWeight)
	{
		for(iC=1;iC<=iClusts;iC++)
		{
			vcEntropy[iC] = ( (prob_t) vCounts[iC] / (prob_t) iNumElems ) * SumEntropy(vDistribs[iC],vZeroes);
		}
	}
	else
	{
		for(iC=1;iC<=iClusts;iC++)
		{
			vcEntropy[iC] = SumEntropy(vDistribs[iC],vZeroes);
		}
	}
}

template< class T >
inline void CalcEntropy(vector< TreeHist<T> >& vDistribs,vector<prob_t>& vcEntropy,int iClusts,vector<int>& vCounts,int iNumElems,bool bWeight)
{
	int iC;
	if(vcEntropy.size() < iClusts+1) vcEntropy = vector<prob_t>(iClusts+1);
	if(bWeight)
	{
		for(iC=1;iC<=iClusts;iC++)
		{
			vcEntropy[iC] = ( (prob_t) vCounts[iC] / (prob_t) iNumElems ) * vDistribs[iC].Entropy();
		}
	}
	else
	{
		for(iC=1;iC<=iClusts;iC++)
		{
			vcEntropy[iC] = vDistribs[iC].Entropy();
		}
	}
}


//gets KLDiv using approximate probabilities
//P(j) = (nj+0.5)/(N+0.5jmax)
//n1+n2+...+nmax=N
//the p distribution's counts are first subtracted
//from the q distribution so that the function will
//calculate the "exclusive" information gain
//p is "partial" distribution
//q is "full" distribution
inline prob_t KLDivApproxExclusiveProb(Hist& p,Hist& q)
{
	if(p.NumBins() != q.NumBins()) return -1.0;

	prob_t d = 0.0;

	int i = 0, iBins = p.NumBins();

	prob_t eps = 1e-100;

	prob_t pelems = p.NumElems();
	prob_t qelems = q.NumElems();

	for(i=0;i<iBins;i++)
	{
		prob_t pcount = p[i];
		prob_t qcount = q[i];

		//approximate probability of bin i in distribution p
		prob_t pp = (pcount+0.5) / ( pelems + 0.5*iBins);

		//approximate probability of bin i in distribution q
		//must subtract pcount elements from numerator & denominator of probability
		//approximation so as to maintain "exclusivity" of p distribution
//		prob_t qp = (qcount - pcount + 0.5) / ( qelems - pcount + 0.5*iBins);
		prob_t qp = (qcount - pcount + 0.5) / ( qelems - pelems + 0.5*iBins);

		if(pp<=eps) continue;

		if(qp<=eps) continue;

		d += pp * ( log2(pp) - log2(qp) );
	}

	return d;
}

inline prob_t FullProb(Hist& h)
{
	prob_t p = 0.0;
	for(int i=0;i<h.NumBins();i++)
	{
		p += h.BinProb(i);
	}
	return p;
}


inline prob_t KLDiv(KDTreeHist& p,KDTreeHist& q)
{	
	int i=0;
#ifdef DO_TIMING
	char sMsg[1024];
	sprintf(sMsg,"KLDiv szp=%d, szq=%d, dims=%d",p.NumElems(),q.NumElems(),p.NumDims());
	ScopedTimer S(sMsg);
#endif
	
	try	//just in case something goes wrong from invalid type of data passed in
	{
		//make sure we can do calculation with div by zero and need same # of dimensions 
		//in each distribution
		if(p.NumElems() < 2 || q.NumElems()<1 || p.NumDims()!=q.NumDims()) return 0.0;

		int iLessP = 0, iLessQ = 0;

		prob_t dpq = 0.0;

		const prob_t eps = 0.0;

		int isz = p.NumElems();

		i = 0;
		for(i=0;i<isz;i++)
		{
			prob_t distp = p.GetNearestRadiusSQ(i);
			
			if(distp<=eps)
				continue;

			prob_t distq = q.GetNearestRadiusSQ(p[i],true);
			
			if(distq<=eps) 
				continue;

			dpq += log2(distq / distp) / 2.0;
		}

		dpq *= ((prob_t)p.NumDims()/p.NumElems());

		dpq += log2( (prob_t)q.NumElems() / (p.NumElems()-1.0 ) );

	#ifdef DO_TIMING
		sprintf(sMsg,"iLessP=%d iLessQ=%d",iLessP,iLessQ);
		MessageBox(0,sMsg,"WClust",MB_ICONINFORMATION);
	#endif
		
		return dpq;
	}
	catch(...)
	{
		char sMsg[1024];
		sprintf(sMsg,"KLDiv caught exception!! i=%d",i);
		MessageBox(0,sMsg,"WClust",MB_ICONERROR);
		return -666.0;
	}
}

//resistor avg.
inline prob_t KLDivSym(KDTreeHist& p,KDTreeHist& q)
{
	prob_t dpq = KLDiv(p,q), dqp = KLDiv(q,p);
	if(!dpq && !dqp) return 0.0;
	return dpq*dqp / (dpq+dqp);
}

template< class T >
inline prob_t KLDiv(TreeHist<T>& p,TreeHist<T>& q)
{
	prob_t d = 0.0;

	int i = 0;

	prob_t eps = 1e-40;

	typename std::map< vpwrap<T>, int>::iterator PIT = p.Begin();
	for(;PIT!=p.End();PIT++)
	{
		prob_t pp = p.GetITProb(PIT);
		if(pp<=eps) 
			continue;
		
		prob_t qp = q.GetVProb(PIT->first.p_);
		
		if(qp<=eps) 
			continue;
		
		d += pp * ( log2(pp) - log2(qp) );
	}
	return d;
}

//returns -1 iff num bins not equal
//otherwise returns KL divergence of histogram/distributions
//this returns "divergence" of distribution q from p
//so q should be a cluster's distribution and p should be full distribution
//make sure when passing in...
//the order matters because it is not symmetric
//however...
//In Bayesian statistics the KL divergence can be used as a measure of the information
//gain in moving from a prior distribution to a posterior distribution. If some new fact
//Y=y is discovered, it can be used to update the probability distribution for X from p(x|I)
//to a new posterior probability distribution p(x|y) using Bayes' theorem
// DKL(p(x|y)||p(x|I)) = sum( p(x|y) * log( p(x|y) / p(x|I) )
//so p can be cluster distribution and q can be full distribution
inline prob_t KLDiv(Hist& p,Hist& q)
{
	if(p.NumBins() != q.NumBins())
	{
		return -1.0;
	}

	prob_t d = 0.0;

	int i = 0 , iBins = p.NumBins();

	prob_t eps = 1e-100;

	for(i=0;i<iBins;i++)
	{
		prob_t pp = p.BinProb(i); //probability of bin i in distribution p
		
		if(pp<=eps) 
			continue;
		
		prob_t qp = q.BinProb(i); //probability of bin i in distribution q
		
		if(qp<=eps) 
			continue;
		
		d += pp * ( log2(pp) - log2(qp) );
	}

/*#ifdef _DEBUG
	if(d<0.0)
	{
		prob_t pf = FullProb(p);
		prob_t qf = FullProb(q);
		FILE* fp = fopen("kl_prob.txt","w");
		fprintf(fp,"p:\n");
		p.Print(fp);
		fprintf(fp,"q:\n");
		q.Print(fp);
		fclose(fp);
		int moo=0;
	}
#endif*/

	return d;
}

//symmetrized form of KL-divergence
inline prob_t ResistorAvg(Hist& p,Hist& q)
{
	prob_t d12 = KLDiv(p,q), d21 = KLDiv(q,p);
	return (d12 * d21) / (d12 + d21);
}

//p is cluster distrib, q is full distrib
inline prob_t Uniqueness(Hist& p,Hist& q)
{
	return KLDiv(p,q);
}

//vZeroes = vector of dimensions that shouldn't contribute to calculations
//vZeroes[i] == 1 , iff you want to exclude dimension i
inline void CalcUDDist(vector< vector<Hist> >& vDistribs,int iClusts,vector<prob_t>& vcInf,vector< vector<prob_t> >& vcInfInter,vector<int>& vCounts,int iElems,vector<int>& vZeroes,bool bUseCounts)
{
	if(vDistribs.size() != iClusts + 2) return;

	vcInf = vector<prob_t>(iClusts+1);

	vcInfInter = vector< vector<prob_t> >(iClusts+1);
	int iC=1;
	for(iC=1;iC<=iClusts;iC++) vcInfInter[iC] = vector<prob_t>(iClusts+1);

	int iDims = vDistribs[1].size() , iD = 0;

	//uniqueness from full distribution for each cluster
	int iC1=1,iC2=1;
	for(iC1=1;iC1<=iClusts;iC1++)
	{
		prob_t kldiv=0.0;
		for(iD=0;iD<iDims;iD++)
		{
			if(vZeroes[iD]) continue;

			kldiv += KLDiv(vDistribs[iC1][iD],vDistribs[iClusts+1][iD]);
		}
		vcInf[iC1] = kldiv;
	}

	//inter-cluster distinction measures, KL divergence between
	//a cluster and cluster+other_cluster
	for(iC1=1;iC1<=iClusts;iC1++)
	{
		for(iC2=1;iC2<=iClusts;iC2++)
		{
			if(iC1==iC2)
				vcInfInter[iC1][iC2]=0.0;
			else
			{
				for(iD=0;iD<iDims;iD++)
				{
					if(vZeroes[iD]) continue;

					Hist oTmp = vDistribs[iC1][iD];
					oTmp.Add(vDistribs[iC2][iD]);
					vcInfInter[iC1][iC2] += KLDiv(vDistribs[iC1][iD],oTmp);
				}
			}				
		}
	}
	//add smallest inter-cluster KL-div
	for(iC1=1;iC1<=iClusts;iC1++)
	{
		prob_t dMinInter = 9e10;
		bool bFound = false;
		if(vCounts[iC1])
		for(iC2=1;iC2<=iClusts;iC2++)
		{
			if(iC1 != iC2 && vCounts[iC2] && vcInfInter[iC1][iC2] < dMinInter)
			{
				dMinInter = vcInfInter[iC1][iC2];
				bFound = true;
			}
		}
		if(bFound)
			vcInf[iC1] += dMinInter;
	}

	if(bUseCounts)
	{
		for(iC1=1;iC1<=iClusts;iC1++)
		{
			vcInf[iC1] = ( (prob_t)vCounts[iC1] / iElems) * vcInf[iC1];
		}
	}
}

//vZeroes = vector of dimensions that shouldn't contribute to calculations
//vZeroes[i] == 1 , iff you want to exclude dimension i
template< class T >
inline void CalcUDDist(vector< TreeHist<T> >& vDistribs,int iClusts,vector<prob_t>& vcInf,vector< vector<prob_t> >& vcInfInter,vector<int>& vCounts,int iElems,vector<int>& vZeroes,bool bUseCounts)
{
	if(vDistribs.size() != iClusts + 2) return;

	vcInf = vector<prob_t>(iClusts+1);

	vcInfInter = vector< vector<prob_t> >(iClusts+1);
	int iC=1;
	for(iC=1;iC<=iClusts;iC++) vcInfInter[iC] = vector<prob_t>(iClusts+1);

	//uniqueness from full distribution for each cluster
	int iC1=1,iC2=1;
	for(iC1=1;iC1<=iClusts;iC1++)
		vcInf[iC1] = KLDiv(vDistribs[iC1],vDistribs[iClusts+1]);

	//inter-cluster distinction measures, KL divergence between
	//a cluster and cluster+other_cluster
	for(iC1=1;iC1<=iClusts;iC1++)
	{
		for(iC2=1;iC2<=iClusts;iC2++)
		{
			if(iC1==iC2)
				vcInfInter[iC1][iC2]=0.0;
			else
			{
				TreeHist<T> oTmp = vDistribs[iC1];
				oTmp.Add(vDistribs[iC2]);
				vcInfInter[iC1][iC2] += KLDiv(vDistribs[iC1],oTmp);
			}				
		}
	}
	//add smallest inter-cluster KL-div
	for(iC1=1;iC1<=iClusts;iC1++)
	{
		prob_t dMinInter = 9e10;
		bool bFound = false;
		if(vCounts[iC1])
		for(iC2=1;iC2<=iClusts;iC2++)
		{
			if(iC1 != iC2 && vCounts[iC2] && vcInfInter[iC1][iC2] < dMinInter)
			{
				dMinInter = vcInfInter[iC1][iC2];
				bFound = true;
			}
		}
		if(bFound)
			vcInf[iC1] += dMinInter;
	}

	if(bUseCounts)
	{
		for(iC1=1;iC1<=iClusts;iC1++)
		{
			vcInf[iC1] = ( (prob_t)vCounts[iC1] / iElems) * vcInf[iC1];
		}
	}
}

inline void CalcUDDist(vector< KDTreeHist >& vDistribs,vector< KDTreeHist >& vCompDistribs,int iClusts,vector<prob_t>& vcInf,vector< vector<prob_t> >& vcInfInter,vector<int>& vCounts)
{
	if(vDistribs.size() != iClusts + 2 || vDistribs.size()!=vCompDistribs.size()) return;

	vcInf = vector<prob_t>(iClusts+1);

	vcInfInter = vector< vector<prob_t> >(iClusts+1);
	int iC=1;
	for(iC=1;iC<=iClusts;iC++) vcInfInter[iC] = vector<prob_t>(iClusts+1);

	//uniqueness from full distribution for each cluster
	int iC1=1,iC2=1;
	for(iC1=1;iC1<=iClusts;iC1++)
		vcInf[iC1] = KLDiv(vDistribs[iC1],vCompDistribs[iC1]);//KLDivSym(vDistribs[iC1],vCompDistribs[iC1]);

	//inter-cluster distinction measures, KL divergence between
	//a cluster and cluster+other_cluster
	for(iC1=1;iC1<=iClusts;iC1++)
	{
		//for(iC2=iC1+1;iC2<=iClusts;iC2++)
		for(iC1=1;iC2<=iClusts;iC2++)
		{
			if(iC1==iC2)
				vcInfInter[iC1][iC2]=0.0;
			else
				//vcInfInter[iC1][iC2] =  vcInfInter[iC2][iC1] = KLDivSym(vDistribs[iC1],vDistribs[iC2]);
				vcInfInter[iC1][iC2] =  KLDiv(vDistribs[iC1],vDistribs[iC2]);
		}
	}
	//add smallest inter-cluster KL-div
	for(iC1=1;iC1<=iClusts;iC1++)
	{
		prob_t dMinInter = 9e10;
		bool bFound = false;
		if(vCounts[iC1])
		for(iC2=1;iC2<=iClusts;iC2++)
		{
			if(iC1 != iC2 && vCounts[iC2] && vcInfInter[iC1][iC2] < dMinInter)
			{
				dMinInter = vcInfInter[iC1][iC2];
				bFound = true;
			}
		}
		if(bFound)
			vcInf[iC1] += dMinInter;
	}
}

//calculates information gain as resistor average of all clusters against full distribution
//plus smallest resistor average between cluster and all other clusters
//vDistribs must have size of iClusts + 1
inline void CalcRDist(vector< vector<Hist> >& vDistribs,int iClusts,vector<prob_t>& vcInf,vector< vector<prob_t> >& vcInfInter)
{
	if(vDistribs.size() != iClusts + 2) return;

	vcInf = vector<prob_t>(iClusts+1);

	vcInfInter = vector< vector<prob_t> >(iClusts+1);
	int iC=1;
	for(iC=0;iC<=iClusts;iC++) vcInfInter[iC] = vector<prob_t>(iClusts+1);

	int iDims = vDistribs[1].size() , iD = 0;

	//only need this for first time, since its recalculated for distributions who's
	//points change, when they change
	int iC1=1,iC2=1;
	for(iC1=1;iC1<=iClusts;iC1++)
	{
		prob_t kldiv=0.0;
		for(iD=0;iD<iDims;iD++)
		{
			kldiv += ResistorAvg(vDistribs[iC1][iD],vDistribs[iClusts+1][iD]);
		}
		vcInf[iC1]=kldiv;
	}

	//fill inter-cluster resistor averages
	for(iC1=1;iC1<=iClusts;iC1++)
	{
		for(iC2=1;iC2<=iC1;iC2++)
		{
			if(iC1==iC2)
				vcInfInter[iC1][iC2]=0.0;
			else
			{
				for(iD=0;iD<iDims;iD++)
				{
					vcInfInter[iC1][iC2]+=ResistorAvg(vDistribs[iC1][iD],vDistribs[iC2][iD]);
				}
				vcInfInter[iC2][iC1]=vcInfInter[iC1][iC2];
			}				
		}
	}
	//add largest inter-cluster resistor average
	for(iC1=1;iC1<=iClusts;iC1++)
	{
		prob_t dMinInter = 9e10;
		bool bFound = false;
		for(iC2=1;iC2<=iClusts;iC2++)
		{
			if(iC1 != iC2 && vcInfInter[iC1][iC2] < dMinInter)
			{
				dMinInter = vcInfInter[iC1][iC2];
				bFound = true;
			}
		}
		if(bFound)
			vcInf[iC1] += dMinInter;
	}
}

template< class T > 
T Sum(vector<T>& v)
{
	int i;
	T val = T(0);
	for(i=0;i<v.size();i++) val += v[i];
	return val;
}

template< class T > 
T Sum(vector<T>& v,int iStart,int iEnd)
{
	int i;
	T val = T(0);
	for(i=iStart;i<v.size() && i<iEnd;i++) val += v[i];
	return val;
}

template< class T > 
T Avg(vector<T>& v)
{
	if(!v.size()) return T(0);
	int i;
	T val = T(0);
	for(i=0;i<v.size();i++) val += v[i];
	return val / (T) v.size();
}

template< class T > 
T Avg(vector<T>& v,int iStart,int iEnd)
{
	if(!v.size()) return T(0);
	int i;
	T val = T(0);
	for(i=iStart;i<iEnd;i++) val += v[i];
	return val / (T) (iEnd-iStart);
}

template< class T >
T Variance(vector<T>& v,int iStart,int iEnd)
{	T avg = Avg(v,iStart,iEnd);
	int i;
	T var = T(0) , val = T(0);
	for(i=iStart;i<iEnd;i++)
	{	val = v[i]-avg;
		var += val*val;
	}
	var /= (T)(iEnd-iStart);
	return var;
}

//get vector containing information gain for each cluster
//std::vector<prob_t> GetClusterInfo(CVerxStack& DataStack,CCluster& MainClusters,CPaletteStack* pMainPal,CFile* pFileBPF,int iBins,int iClusts,int which);
//get string rep of cluster info gain for writing to bpf,cl files
CString GetClusterInfoString(CVerxStack& MainDataStack,CCluster& MainClusters,HWND wnd=0);

bool PrintDistribs(vector<vector< Hist> >& vDistribs,char* fname);

bool Distribs2Images(vector< vector< Hist > >& vDistribs, char* fname_base);

bool Distribs2Matlab(vector< vector< Hist > >& vDistribs, char* fname_base);

void FillDistribs(CVerxStack& DataStack,CCluster& MainClusters,int iBins,std::vector< std::vector<Hist> >& vDistribs,int iClusts,int which);

//this is the continuous multidimensional probability version
inline void FillDistribs(vector<float>& vFloat,vector<KDTreeHist>& vDistribs,vector<KDTreeHist>& vCompDistribs,int iDistribs,vector<int>& vClustIDs,vector<int>& vCounts,int iDims,bool bGetComplements)
{
	vDistribs = vector< KDTreeHist >(iDistribs+1);

	if(bGetComplements)
		vCompDistribs = vector< KDTreeHist >(iDistribs+1);

	int iV = 0 , iC = 0;

	//full distribution
	vDistribs[iDistribs].SetData(iDims,&vFloat[0],vFloat.size()/iDims);

	int iTotalVs = vFloat.size() / iDims;

	for(iC=1;iC<iDistribs;iC++)
	{
		vector<float> vClustData(vCounts[iC]*iDims), vCompData;
		int iCompSize = iTotalVs - vCounts[iC];
		if(bGetComplements) vCompData = vector<float>(iCompSize*iDims);
		int j = 0 , k = 0;
		for(iV=0;iV<vClustIDs.size();iV++)
		{
			if(vClustIDs[iV] == iC)
			{
				int iD = 0;
				for(iD=0;iD<iDims;iD++)
				{
					vClustData[j++]=vFloat[iV*iDims+iD];
				}
			}
			else if(bGetComplements)
			{
				int iD = 0;
				for(iD=0;iD<iDims;iD++)
				{
					vCompData[k++]=vFloat[iV*iDims+iD];
				}
			}
		}
		vDistribs[iC].SetData(iDims,&vClustData[0],vCounts[iC]);
		if(bGetComplements)
			vCompDistribs[iC].SetData(iDims,&vCompData[0],iCompSize);
	}
}


template< class T >
void FillDistribs(CVerxStack& DataStack,vector< vector<int> >& vBinData,CCluster& MainClusters,int iBins,vector< TreeHist<T> >& vDistribs,int iDistribs,vector<int>& vClustIDs)
{
	//distrib for each cluster + 1 for full distrib
	vDistribs = std::vector< TreeHist<T> >(iDistribs+1);
		
	int iV = 0;
	for(iV=0;iV<vClustIDs.size();iV++)
	{		
		//cluster spike belongs to
		vDistribs[vClustIDs[iV]].UpdateProb(&vBinData[iV],1);

		//FULL distribution containing all spikes!!
		vDistribs[iDistribs].UpdateProb(&vBinData[iV],1);
	}
}

inline void FillDistribs(CVerxStack& DataStack,vector< vector<float>* >& vFloatps,CCluster& MainClusters,int iBins,vector< TreeHist<float> >& vDistribs,int iDistribs,vector<int>& vClustIDs,int iMinD,int iMaxD)
{
	//distrib for each cluster + 1 for full distrib
	vDistribs = std::vector< TreeHist<float> >(iDistribs+1);

	int iC=1;
	for(iC=1;iC<=iDistribs;iC++)
	{
		vDistribs[iC].SetBinless(true);
		vDistribs[iC].SetDRange(iMinD,iMaxD);
	}
		
	int iV = 0;
	for(iV=0;iV<vClustIDs.size();iV++)
	{		
		//cluster spike belongs to
		vDistribs[vClustIDs[iV]].UpdateProb(vFloatps[iV],1);

		//FULL distribution containing all spikes!!
		vDistribs[iDistribs].UpdateProb(vFloatps[iV],1);
	}
}


template< class T >
void FillDistribs(CVerxStack& DataStack,CCluster& MainClusters,int iBins,vector< TreeHist<T> >& vDistribs,int iDistribs,int which,vector<vector<int> >& vBinIDs)
{
	//distrib for each cluster + 1 for full distrib
	vDistribs = vector< TreeHist<T> >(iDistribs+1);
	int iDims = DataStack.GetAutoClusteringDimension();
	
	int iV=0,iC=1; 
	
	//go through vertices
	MY_STACK::iterator Index;
	for (Index=DataStack.m_VerxStack.begin();Index!=DataStack.m_VerxStack.end();Index++,iV++)
	{	
		CVertex* verx = (CVertex*)*Index;

		//skip noise
		if(verx->GetNoise()) continue;
			
		//go through clusters filling out distrib info
		for(iC=1;iC<=iDistribs;iC++)
		{
			//either spike is in cluster or it is the FULL distribution
			//containing all spikes!!
			if(iC==iDistribs || GetVClust(verx,which)==iC)
			{
				vDistribs[iC].UpdateProb(&vBinIDs[iV],1);
			}
		}
	}
}

void FillDistribs(CVerxStack& DataStack,int** pData,CCluster& MainClusters,int iBins,std::vector< std::vector<Hist> >& vDistribs,int iDistribs,vector<int>& vClustIDs);

bool RandAssign(CVerxStack& DataStack,CCluster& MainClusters,int iClusts,int which);

inline char GetVClust(CVertex* verx,int which)
{
	switch(which)
	{
	case CLUST_USER:
		return verx->GetClust();
		break;
	case CLUST_ORIG:
		return verx->GetOrigClust();
		break;
	case CLUST_KM:
		return verx->GetKmeansClust();
		break;
	case CLUST_INFO:
		return verx->GetInfoClust();
		break;
	case CLUST_AP:
		return verx->GetAPClust();
		break;
	case CLUST_KK:
		return verx->GetKKClust();
		break;
	}
	return 0;
}
@


1.11
log
@adjust to latest kdtree which doesn't take sqrt of points before returning them. all values returned are now squared distances. for logs just do log / 2 . when really need distances must take their sqrt
@
text
@d1 1
a1 1
// $Id: Hist.h,v 1.10 2008/03/05 01:25:32 samn Exp $ 
d7 1
d230 1
a230 1
			m_pTree->search_center_radius_sq(p,fRad);
d232 1
a232 1
			if(iCount>iNNToFind)
d242 4
d285 1
a285 1
			m_pTree->search_center_radius_sq(p,fRad);
d318 1
a318 1
			m_pTree->search_center_radius_sq(p,fRad);
d359 1
a359 1
			m_pTree->search_center_radius_sq(p,fRad);
@


1.10
log
@use smaller starting radius in GetKNN (as determined in CalcClusterInfo in fast mode of kldiv) -- need to make sure it is set properly in non-fast mode too!
@
text
@d1 1
a1 1
// $Id: Hist.h,v 1.9 2008/03/03 20:10:00 samn Exp $ 
d229 1
a229 1
			m_pTree->search_center_radius(p,fRad);
d234 1
a234 1
				m_pTree->copy_radii(&vRadii[0]);
d280 1
a280 1
			m_pTree->search_center_radius(p,fRad);
d285 1
a285 1
				m_pTree->copy_radii(&vRadii[0]);
d303 1
a303 1
	float GetNearestRadius(float* p,vector<int>& vMap,int iID)
a306 3
		//const int iNumRads = 7;
		//float pRads[iNumRads] = {3.0f,30.0f,150.0f,300.0f,600.0f,900.0f,1000.0f};

a307 1
		//float fRad = 15.0f ;//pRads[0];
d313 1
a313 1
			m_pTree->search_center_radius(p,fRad);
d318 1
a318 1
				m_pTree->copy_radii(&vRadii[0]);
d340 1
a340 4
			//if(iIter+1 >= iNumRads)
				fRad *= two;
			//else
			//	fRad = pRads[++iIter];
d344 1
a344 1
	float GetNearestRadius(float* p,bool bAllowZeroDist)
d346 1
a346 4
		if(m_iNumElems == 1) return 0.0;
		
		//const int iNumRads = 7;
		//float pRads[7] = {3.0f,30.0f,150.0f,300.0f,600.0f,900.0f,1000.0f};
a348 1
		//float fRad = pRads[0];
d354 1
a354 1
			m_pTree->search_center_radius(p,fRad);
d359 1
a359 1
				m_pTree->copy_radii(&vRadii[0]);
d372 1
a372 4
			//if(iIter+1 >= iNumRads)
				fRad *= two;
			//else
			//	fRad = pRads[++iIter];
d376 1
a376 1
	float GetNearestRadius(int i)
d378 1
a378 1
		return GetNearestRadius(&m_vData[i*m_iDims],false);
d415 1
a415 1
		prob_t dRad = GetNearestRadius(p,false);
d428 1
a428 1
		prob_t dRad = GetNearestRadius(p,true);
d450 1
a450 1
			prob_t dDist = GetNearestRadius(&m_vData[iOffset],false);
d452 1
d500 1
a500 1
				prob_t dDist = GetNearestRadius(&m_vData[i*m_iDims],vIDs,iClust);
d502 1
d1165 1
a1165 1
			prob_t distp = p.GetNearestRadius(i);
d1170 1
a1170 1
			prob_t distq = q.GetNearestRadius(p[i],true);
d1175 1
a1175 6
			//if(distq<distp)
			//	iLessQ++;
			//else
			//	iLessP++;

			dpq += log2(distq / distp);
a1185 5
		//if(dpq<0.0)
		//{	char sMsg[1024];
		//	sprintf(sMsg,"iLessP=%d iLessQ=%d",iLessP,iLessQ);
		//	MessageBox(0,sMsg,"WClust",MB_ICONERROR);
		//}
@


1.9
log
@save some memory/time when getting nearest neighbors
@
text
@d1 1
a1 1
// $Id: Hist.h,v 1.8 2008/02/02 21:22:52 samn Exp $ 
d220 1
a220 1
	void GetKNN(float* p,vector<Neighbor>& vnn,int iNNToFind,double fRadStart=8.0,double fRadFctr=2.0)
d222 1
a222 1
		if(m_iNumElems == 1) return;
d260 1
a260 1
				return;
d311 3
a313 1
		float fRad = 15.0f ;//pRads[0];
d355 2
a356 2
		const int iNumRads = 7;
		float pRads[7] = {3.0f,30.0f,150.0f,300.0f,600.0f,900.0f,1000.0f};
d359 3
a361 1
		float fRad = pRads[0];
d383 1
a383 1
			if(iIter+1 >= iNumRads)
d385 2
a386 2
			else
				fRad = pRads[++iIter];
d1602 23
@


1.8
log
@added ProbInitFree struct to initialize and free mem of probability lookup tables
@
text
@d1 1
a1 1
// $Id: Hist.h,v 1.7 2008/01/13 02:45:39 samn Exp $ 
d239 3
a241 1
				int i = 0;
d244 3
a246 2
						vnn.push_back(Neighbor(vID[i],vRadii[i]));
								
d249 6
a254 1
				if(vnn.size()>iNNToFind) vnn.resize(iNNToFind);
@


1.7
log
@added Sum with range
@
text
@d1 1
a1 1
// $Id: Hist.h,v 1.6 2008/01/12 09:36:41 samn Exp $ 
d23 6
d173 2
a1147 4
	//make sure we can do calculation with div by zero and need same # of dimensions 
	//in each distribution
	if(p.NumElems() < 2 || q.NumElems()<1 || p.NumDims()!=q.NumDims()) return 0.0;

d1150 4
@


1.6
log
@added some useful functions to KDTreeHist
@
text
@d1 1
a1 1
// $Id$ 
d1564 9
@


1.5
log
@change KLDivSym to be resistor average instead of regular average, check for exceptions in KLDiv
@
text
@d66 14
d110 59
d174 1
d212 39
d287 46
d429 2
d433 4
a436 1
	{
d438 4
a441 1
		int isz = m_iNumElems , i=0;
d444 4
a447 2
			prob_t dProb = IProb(i);
			if(dProb==0.0) continue;
d449 1
d472 30
@


1.4
log
@display error message for clusters with kldiv < 0 telling # of times nearest neighbor of point in p is in p and number of times it is in q (iLessP means nearest neighbor of point from p is in p)
@
text
@d932 2
a933 1
{
d939 8
a946 1
	int iLessP = 0, iLessQ = 0;
d948 1
a948 1
	prob_t dpq = 0.0;
d950 1
a950 1
	prob_t eps = 0.0; //1e-40;
d952 9
a960 1
	int isz = p.NumElems();
d962 4
a965 7
	int i = 0;
	for(i=0;i<isz;i++)
	{
		prob_t distp = p.GetNearestRadius(i);
		
		if(distp<=eps)
			continue;
d967 4
a970 4
		prob_t distq = q.GetNearestRadius(p[i],true);
		
		if(distq<=eps) 
			continue;
d972 1
a972 22
		if(distq<distp)
		{
			iLessQ++;
#ifdef DO_TIMING
#ifdef _DEBUG
			float* pt = p[i];
			float* pt2 = p.GetNearestNeighbor(p[i],false);
			float* qt = q.GetNearestNeighbor(p[i],true);
			float sqd = 0 , sqd2 = 0;
			int t = 0;
			for(;t<p.NumDims();t++)
			{
				float tv = pt[t]-qt[t];
				sqd += tv*tv;
				tv = pt[t]-pt2[t];
				sqd2 += tv*tv;
			}
			float ddd = sqrt(sqd);
			float ddd2 = sqrt(sqd2);
			int moo=0;
#endif
#endif
a973 4
		else iLessP++;

		dpq += log2(distq / distp);
	}
d975 1
a975 1
	dpq *= ((prob_t)p.NumDims()/p.NumElems());
d977 1
a977 1
	dpq += log2( (prob_t)q.NumElems() / (p.NumElems()-1.0 ) );
d979 1
a979 6
#ifdef DO_TIMING
	sprintf(sMsg,"iLessP=%d iLessQ=%d",iLessP,iLessQ);
	MessageBox(0,sMsg,"WClust",MB_ICONINFORMATION);
#endif
	if(dpq<0.0)
	{	char sMsg[1024];
d981 14
d996 1
a997 2
	
	return dpq;
d1000 1
d1003 3
a1005 1
	return (KLDiv(p,q) + KLDiv(q,p)) / 2.0;
d1376 1
a1376 1
CString GetClusterInfoString(CVerxStack& MainDataStack,CCluster& MainClusters);
@


1.3
log
@less message boxes for now, since less debugging for time being
@
text
@d937 1
a938 1
#endif
a958 1
#ifdef DO_TIMING
d962 1
d980 1
a982 1
#endif
d995 5
@


1.2
log
@added debug code, added nearestneighbor for kdtreehist, fixed kldiv for continuous distribution to get > 0 val
@
text
@d1 1
a1 1
/* $Id: Hist.h,v 1.1 2008/01/05 15:00:52 samn Exp $ */
d10 1
a10 1
#define DO_TIMING
d959 1
d982 1
a983 1
		//dpq += (log2(distq) - log2(distp));
a988 2
	//dpq += (log2((prob_t)q.NumElems()) - log2((prob_t)p.NumElems()-1.0));

@


1.1
log
@Initial revision
@
text
@d1 1
a1 1
/* $Id$ */
d28 40
d138 36
d193 1
a193 1
				vector<float>::iterator IT;
a196 17
				else if(iCount > 1)
				{	IT = std::min_element(vRadii.begin(),vRadii.end(),GZeroMinElem);
					if(*IT <= 0.0)
					{
						char msg[1024]; sprintf(msg,"*********** fRad=%.1f, bAllowZeroDist=%s, iCount=%d, dist_size=%d, *IT=%.2f",fRad,bAllowZeroDist?"true":"false",iCount,m_iNumElems,*IT);
						MessageBox(0,msg,"WClust",0);
						char tmpStr[16384];
						sprintf(tmpStr,"vRadii: ");
						int k;
						for(k=0;k<vRadii.size() && k < 20;k++)
						{
							sprintf(msg,"R[%d]=%.2f ",k,vRadii[k]);
							strcat(tmpStr,msg);
						}
						MessageBox(0,tmpStr,"WClust",0);
					}
				}
d198 1
a198 1
					IT = std::min_element(vRadii.begin(),vRadii.end(),GZeroMinElem);
d200 2
a201 16
				if(false && (fRad > 512.0 || iCount==m_iNumElems))
				{	char msg[1024]; sprintf(msg,"fRad=%.1f, bAllowZeroDist=%s, iCount=%d, dist_size=%d, *IT=%.2f",fRad,bAllowZeroDist?"true":"false",iCount,m_iNumElems,*IT);
					MessageBox(0,msg,"WClust",0);
					char tmpStr[16384];
					sprintf(tmpStr,"vRadii: ");
					int k;
					for(k=0;k<vRadii.size() && k < 20;k++)
					{
						sprintf(msg,"R[%d]=%.2f ",k,vRadii[k]);
						strcat(tmpStr,msg);
					}
					MessageBox(0,tmpStr,"WClust",0);
				}

				if(*IT > 0.0)
					return *IT;
d213 1
a213 1
		return GetNearestRadius(&m_vData[i],false);
d937 1
d942 1
a942 1
	prob_t eps = 1e-40;
d959 23
d991 5
d1383 1
@
